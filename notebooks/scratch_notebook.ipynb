{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d21af80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Union, NamedTuple, Any, Sequence, Callable, override, overload\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from collections.abc import Generator, MutableMapping, Mapping, Sequence\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import auc as sk_auc\n",
    "from scipy.sparse import coo_matrix\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from argparse import ArgumentParser, Namespace\n",
    "from pathlib import Path\n",
    "from functools import reduce\n",
    "\n",
    "from pydantic import validate_call\n",
    "from tqdm import tqdm\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import statistics\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import logging\n",
    "import sys\n",
    "import copy\n",
    "import re\n",
    "\n",
    "import torch\n",
    "\n",
    "from hierarchy_transformers import HierarchyTransformer\n",
    "from OnT.OnT import OntologyTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7289284a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: json2html in /root/miniconda/envs/knowledge-retrieval-env/lib/python3.12/site-packages (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "VERBOSE = False\n",
    "! pip install json2html\n",
    "\n",
    "import json2html\n",
    "import latextable\n",
    "from latextable import texttable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaafd78f",
   "metadata": {},
   "source": [
    "## Basic Utils\n",
    "\n",
    "*Source: [data_utils.py](../src/hroov/utils/data_utils.py)*\n",
    "\n",
    "Small helper functions used for tokenisation and stripping higher level concept tags (e.g. '$Hear\\ Disease\\ (Condition) \\rightarrow Heart\\ Disease$') for leakage prevention.\n",
    "\n",
    "TODO: update description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5db89aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "_regex_parens = re.compile(r\"\\s*\\([^)]*\\)\") # for parentheses removal (prevents leakage)\n",
    "\n",
    "def strip_parens(s: str) -> str:\n",
    "    return _regex_parens.sub(\"\", s)\n",
    "\n",
    "def load_json(file_path: Path) -> dict[str, str]:\n",
    "    with file_path.open('r', encoding='utf-8') as fp:\n",
    "        return json.load(fp)\n",
    "\n",
    "def save_json(file_path: Path, payload: dict | list, encoding: str = \"utf-8\", indentation: int = 4) -> None:\n",
    "    with open(file_path, \"w\", encoding=encoding) as fp:\n",
    "        json.dump(payload, fp, indent=indentation)\n",
    "\n",
    "def load_concepts_to_list(concepts_file_path: Path) -> list[str]:\n",
    "    return list(load_json(concepts_file_path).values())\n",
    "\n",
    "def naive_tokenise(seq: str) -> list[str]:\n",
    "    return seq.lower().split()\n",
    "\n",
    "def parallel_tokenise(seq_list: list[str], workers: int, chunksize: int = 25000) -> list[list[str]]:\n",
    "    with Pool(workers) as pool:\n",
    "        return list(pool.map(naive_tokenise, seq_list, chunksize=chunksize))\n",
    "    \n",
    "def produce_candidate_ids_for_embs(embeddings_ds):\n",
    "    return np.arange(len(embeddings_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d5733c",
   "metadata": {},
   "source": [
    "## Indexing Utils\n",
    "\n",
    "*Source: [retrievers.py](../src/hroov/utils/retrievers.py)*\n",
    "\n",
    "TODO: add descripton here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7c85c28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted TF-IDF at run-time\n",
    "def aggregate_posting_scores(query_weights, inverted):\n",
    "    scores = {}\n",
    "    for term, weight in query_weights.items():\n",
    "        if term not in inverted:\n",
    "            continue\n",
    "        for doc_id, tfidf_score in inverted[term]:\n",
    "            scores[doc_id] = scores.get(doc_id, 0.0) + weight * tfidf_score\n",
    "    return scores\n",
    "\n",
    "# sort TF-IDF result set\n",
    "def topk(scores: dict[int, float], k: int = 10):\n",
    "    return sorted(scores.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "def build_tf_idf_index(axiom_list: list[str], tfidf_dest: str, *args, **kwargs):\n",
    "    vectoriser = TfidfVectorizer(**kwargs)\n",
    "    doc_term_matrix = vectoriser.fit_transform(axiom_list)\n",
    "    vocab = vectoriser.get_feature_names_out()\n",
    "    # prep for storing to disk: create empty postings struct\n",
    "    inverted_index: dict[str, list[tuple[int, float]]] = {term: [] for term in vocab}\n",
    "    # see: https://matteding.github.io/2019/04/25/sparse-matrices/\n",
    "    coo = coo_matrix(doc_term_matrix)\n",
    "    # populate the inverted index\n",
    "    for row, col, score in zip(coo.row, coo.col, coo.data):\n",
    "        inverted_index[str(vocab[col])].append((int(row), float(score)))\n",
    "    # order: desc\n",
    "    for postings in inverted_index.values():\n",
    "        postings.sort(key=lambda x: x[1], reverse=True)\n",
    "    # save to disk\n",
    "    with open(tfidf_dest, \"wb\") as fp:\n",
    "        pickle.dump(\n",
    "        {\n",
    "            \"vectorizer\": vectoriser,\n",
    "            \"postings\": postings, # type: ignore\n",
    "            \"verbalisations\": axiom_list\n",
    "        },\n",
    "        fp,\n",
    "        protocol=pickle.HIGHEST_PROTOCOL,\n",
    "    )\n",
    "    return vectoriser, inverted_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a7abe2",
   "metadata": {},
   "source": [
    "<style>\n",
    "*{\n",
    "    line-height: 24px;\n",
    "}\n",
    "</style>\n",
    "\n",
    "## Math Functions\n",
    "\n",
    "*Source: [math_functools.py](../src/hroov/utils/math_functools.py)*\n",
    "\n",
    "TODO: add description.\n",
    "\n",
    "**L2 distance:**\n",
    "\n",
    "$$ L_2 = \\| v - u \\|_2 \\equiv \\sqrt{\\sum_{i=1}^d (v_i - u_i)^2} $$\n",
    "\n",
    "**Inner product:**\n",
    "\n",
    "$$\\sum_{i=1}^{d} u_i \\cdot v_i $$\n",
    "\n",
    "**Cosine Similarity:**\n",
    "\n",
    "$$ sim(u,v) = \\operatorname{cos}(\\theta) = \\frac{u \\cdot v }{\\| u \\|_2 \\| v \\|_2} $$\n",
    "\n",
    "**Geodesic Distance (\\w adaptive/sectional curvature $\\kappa$):**\n",
    "\n",
    "$$\n",
    "d_{\\kappa}(u,v) = \\frac{1}{\\sqrt{\\kappa}} \n",
    "\\cdot \n",
    "\\operatorname{arcosh} \n",
    "\\Biggl( 1 + \\frac{2\\kappa \\|u - v\\|^{2}}{\n",
    "  \\bigl( 1 - \\kappa \\|u\\|^{2} \\bigr) \\cdot \\bigl( 1 - \\kappa \\|v\\|^{2} \\bigr)\n",
    "}\n",
    "\\Biggr),\n",
    "\\qquad\n",
    "\\|u\\|,\\|v\\|<\\frac{1}{\\sqrt{\\kappa}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f2c0543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_euclidian_l2_distance(u: np.ndarray, vs: np.ndarray) -> np.ndarray:\n",
    "    return np.linalg.norm(u - vs, axis=1)\n",
    "\n",
    "def l2_norm(x: np.ndarray) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=np.float32)\n",
    "    return np.sqrt(np.sum(x**2))\n",
    "\n",
    "def batch_l2_norm(x: np.ndarray) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=np.float32)\n",
    "    return np.asarray(np.sqrt(np.sum(x**2, axis=1)))\n",
    "\n",
    "def inner_product(p_u: np.ndarray, p_v: np.ndarray) -> np.ndarray:\n",
    "    u = np.asarray(p_u, dtype=np.float32)\n",
    "    v = np.asarray(p_v, dtype=np.float32)\n",
    "    return np.inner(u, v)\n",
    "\n",
    "def batch_inner_product(p_u: np.ndarray, p_vs: np.ndarray) -> np.ndarray:\n",
    "    u = np.asarray(p_u, dtype=np.float32).ravel()\n",
    "    vs = np.asarray(p_vs, dtype=np.float32)\n",
    "    return vs.dot(u)\n",
    "\n",
    "def cosine_similarity(u, v, normalised=True):\n",
    "    u = np.asarray(u, dtype=np.float32)\n",
    "    v = np.asarray(v, dtype=np.float32)\n",
    "    return np.inner(u, v) if normalised else np.inner(u, v) / (l2_norm(u) * l2_norm(v))\n",
    "\n",
    "def batch_cosine_similarity(p_u, p_vs, normalised=True):\n",
    "    u  = np.asarray(p_u,  dtype=np.float32)\n",
    "    vs = np.asarray(p_vs, dtype=np.float32)\n",
    "    return batch_inner_product(u, vs) if normalised else batch_inner_product(u, vs) / (l2_norm(u) * batch_l2_norm(vs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190dcb39",
   "metadata": {},
   "source": [
    "<style>\n",
    "*{\n",
    "    line-height: 24px;\n",
    "}\n",
    "</style>\n",
    "\n",
    "## Hyperbolic Space\n",
    "\n",
    "A Riemannian manifold $\\mathcal{M}$, of dimension $d$ is defined as a smooth (differentiable) manifold equipped with a Riemannian metric tensor $g$, such that the manifold is represented by $(\\mathcal{M}, g)$. For any point $x \\in \\mathcal{M}$, there exists a local neighbourhood whose geometry resembles Euclidean geometry. Hyperbolic space $\\mathbb{H}^n$ is a Riemannian manifold with a constant sectional curvature $-\\kappa$, which can be represented in the Poincaré ball model whose points lie within the open ball, given by:\n",
    " \n",
    "$$\n",
    "B_{\\kappa}^n = \\{\\ x \\in \\mathbb{R}^n : \\|x\\| < r\\ \\}, \\qquad r=\\frac{1}{\\sqrt{\\kappa}},\n",
    "$$\n",
    "\n",
    "where $r$ is the radius of the ball. The Poincaré metric $g_{\\kappa}$ induces the hyperbolic distance function $d_{\\kappa}$ between any two points $x,y \\in B^n_{\\kappa}$, applied for scoring in §\\ref{sec:task-definition-and-methodology}, is given by:\n",
    "\n",
    "\n",
    "$$\n",
    "d_{\\kappa}(x,y) = \\frac{1}{\\sqrt{\\kappa}} \n",
    "\\cdot \n",
    "\\operatorname{arcosh} \n",
    "\\Biggl( 1 + \\frac{2\\kappa \\|x - y\\|^{2}}{\n",
    "  \\bigl( 1 - \\kappa \\|x\\|^{2} \\bigr) \\cdot \\bigl( 1 - \\kappa \\|y\\|^{2} \\bigr)\n",
    "}\n",
    "\\Biggr),\n",
    "\\qquad\n",
    "\\|x\\|,\\|y\\|<\\frac{1}{\\sqrt{\\kappa}}.\n",
    "$$\n",
    "\n",
    "As $\\|x\\|$ and $\\|y\\|$ approach the boundary of the ball (norm $\\to \\frac{1}{\\sqrt{\\kappa}}$), distances diverge even if the Euclidean norm difference $\\|x-y\\|$ is not itself significant, meaning that points situated near the boundary can represent more specific concepts (since their hyperbolic separation becomes large). This is in contrast to points situated toward the center, that represent more generic concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a20a0011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_poincare_distance_with_curv_k(u: np.ndarray, vs: np.ndarray, k: np.float64 | np.float32) -> np.float64 | np.float32:\n",
    "    u_norm_sqd = np.sum(u**2)\n",
    "    vs_norms_sqd = np.sum(vs**2, axis=1)\n",
    "    l2_dist_sqd = np.sum((u - vs)**2, axis=1)\n",
    "    offset = 1e-7 # tiny-offset: guard agaisnt division by zero & floating point arithmatic inaccuracies\n",
    "    arg = 1 + ((2 * k * l2_dist_sqd) / ((1 - (k * u_norm_sqd + offset)) * (1 - (k * vs_norms_sqd + offset)))) # acosh\n",
    "    arg = np.maximum(1.0, arg) # bounds check: domain of acosh is bound to [1, \\inf)\n",
    "    acosh_scaling = np.float64(1) / np.float64(np.sqrt(k)) # scaling factor: k\n",
    "    return (acosh_scaling * np.arccosh(arg, dtype=np.float64)) # 1 / sqrt(k) * acosh(arg)\n",
    "\n",
    "def batch_poincare_dist_with_adaptive_curv_k(u: np.ndarray, vs:np.ndarray, model: HierarchyTransformer | OntologyTransformer, **kwargs):\n",
    "    if isinstance(model, HierarchyTransformer):\n",
    "        k = np.float64(model.get_circum_poincareball(model.embed_dim).c)\n",
    "    elif isinstance(model, OntologyTransformer):\n",
    "        hierarchy_model = model.hit_model\n",
    "        hierarchy_poincare_ball = hierarchy_model.get_circum_poincareball(hierarchy_model.embed_dim)\n",
    "        k = np.float64(hierarchy_poincare_ball.c)    \n",
    "    else:\n",
    "        raise Exception(\"Hyperbolic distance should only be only calculated in B^n or H^n\")\n",
    "    return np.asarray(batch_poincare_distance_with_curv_k(u, vs, k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f1c573",
   "metadata": {},
   "source": [
    "<style>\n",
    "*{\n",
    "    line-height: 24px;\n",
    "}\n",
    "</style>\n",
    "\n",
    "## Subsumption Score\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f654ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "def subsumption_score_hit(hit_transformer: HierarchyTransformer, child_emb: np.ndarray | torch.Tensor, parent_emd: np.ndarray | torch.Tensor, centri_weight: float = 1.0):\n",
    "    child_emb_t = torch.Tensor(child_emb)\n",
    "    parent_emb_t = torch.Tensor(parent_emd)\n",
    "    dists = hit_transformer.manifold.dist(child_emb_t, parent_emb_t)\n",
    "    child_norms = hit_transformer.manifold.dist0(child_emb_t)\n",
    "    parent_norms = hit_transformer.manifold.dist0(parent_emb_t)\n",
    "    return -(dists + centri_weight * (parent_norms - child_norms))\n",
    "\n",
    "def subsumption_score_ont(ontology_transformer: OntologyTransformer, child_emb: np.ndarray | torch.Tensor, parent_emb: np.ndarray | torch.Tensor, weight_lambda: float = 1.0):\n",
    "    child_emb_t = torch.Tensor(child_emb)\n",
    "    parent_emb_t = torch.Tensor(parent_emb)\n",
    "    return ontology_transformer.score_hierarchy(child_emb_t, parent_emb_t, weight_lambda)\n",
    "\n",
    "def entity_subsumption(u: np.ndarray, vs: np.ndarray, model: HierarchyTransformer, *, weight: float = 0.35):\n",
    "    return np.asarray(subsumption_score_hit(model, u, vs, centri_weight=weight))\n",
    "\n",
    "def concept_subsumption(u: np.ndarray, vs: np.ndarray, model: OntologyTransformer, *, weight: float = 0.35, **kwargs):\n",
    "    return np.asarray(subsumption_score_ont(model, u, vs, weight_lambda=weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a125815",
   "metadata": {},
   "source": [
    "<style>\n",
    "*{\n",
    "    line-height: 24px;\n",
    "}\n",
    "</style>\n",
    "\n",
    "## Data Mapping & Management\n",
    "\n",
    "*Source: [query_utils.py](../src/hroov/utils/query_utils.py)*\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54c652db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_signature(obj):\n",
    "  if isinstance(obj, Mapping):\n",
    "    return tuple((key, make_signature(obj[key])) for key in sorted(obj))\n",
    "  elif isinstance(obj, Sequence) and not isinstance(obj, (str, bytes, bytearray)):\n",
    "    return tuple(make_signature(item) for item in obj)\n",
    "  # else:\n",
    "  return obj\n",
    "\n",
    "def unique_unhashable_object_list(obj_xs: list[dict]) -> list[dict]:\n",
    "    object_signatures = set()\n",
    "    unique_obj_list = []\n",
    "    for obj in obj_xs:\n",
    "      signature = make_signature(sorted(obj.items()))\n",
    "      if signature not in object_signatures:\n",
    "        object_signatures.add(signature)\n",
    "        unique_obj_list.append(obj)\n",
    "    return unique_obj_list\n",
    "\n",
    "def obj_max_depth(x: int, obj: Any, key: str = \"depth\") -> int:\n",
    "  return x if x > obj[key] else obj[key]\n",
    "\n",
    "def dcg_exp_relevancy_at_pos(relevancy: int, rank_position: int) -> float:\n",
    "  if relevancy <= 0:\n",
    "    return float(0.0)\n",
    "  numerator = (2**relevancy) - 1\n",
    "  denominator = math.log2(rank_position + 1)\n",
    "  return float(numerator / denominator)\n",
    "\n",
    "def dcg_linear_relevancy_at_pos(relevancy: int, rank_position: int) -> float:\n",
    "  if relevancy <= 0:\n",
    "    return float(0.0)\n",
    "  numerator = relevancy\n",
    "  denominator = math.log2(rank_position + 1)\n",
    "  return float(numerator / denominator)\n",
    "\n",
    "def accumulate(a, b, key='dcg'):\n",
    "  return a + b[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5d1442",
   "metadata": {},
   "source": [
    "#### Data Mapping: Query Models\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cbcab4",
   "metadata": {},
   "source": [
    "**Query Base Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64f655ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Query:\n",
    "  \n",
    "  _query_obj_repr: dict\n",
    "  _query_string: str\n",
    "  _target: dict[str, Union[str, int]]\n",
    "  _entity_mention: dict[str, Union[str, int]]\n",
    "\n",
    "  def __init__(self, query_obj_repr: dict):\n",
    "    self._query_obj_repr = query_obj_repr\n",
    "    self._query_string = query_obj_repr['entity_mention']['entity_literal']\n",
    "    self._target = query_obj_repr['target_entity']\n",
    "    self._entity_mention = query_obj_repr['entity_mention']\n",
    "\n",
    "  def get_query_string(self) -> str:\n",
    "    return self._query_string\n",
    "  \n",
    "  def get_target_iri(self) -> str:\n",
    "    return str(self._target['iri'])\n",
    "  \n",
    "  def get_target_label(self) -> str:\n",
    "    return str(self._target['rdfs:label'])\n",
    "  \n",
    "  def get_target_depth(self) -> int:\n",
    "    return int(self._target['depth'])\n",
    "  \n",
    "  def get_target(self) -> dict:\n",
    "    return self._target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cbfa01",
   "metadata": {},
   "source": [
    "**Equivalent Query Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad6b5883",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EquivQuery(Query):\n",
    "  \n",
    "  _equiv_class_expression: Union[str, None]\n",
    "\n",
    "  def __init__(self, query_obj_repr: dict):\n",
    "    super().__init__(query_obj_repr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78063ded",
   "metadata": {},
   "source": [
    "**Subsumption Query Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8477997",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubsumptionQuery(Query):\n",
    "  \n",
    "  _parents: list\n",
    "  _ancestors: list\n",
    "\n",
    "  def __init__(self, query_obj_repr: dict):\n",
    "    super().__init__(query_obj_repr)\n",
    "    self._set_parents()\n",
    "    self._set_ancestors()\n",
    "\n",
    "  def _set_parents(self):\n",
    "    if self._query_obj_repr['parent_entities'] and len(self._query_obj_repr['parent_entities']) > 0:\n",
    "      self._parents = self._query_obj_repr['parent_entities']\n",
    "    else:\n",
    "      self._parents = []\n",
    "\n",
    "  def _set_ancestors(self):\n",
    "    if self._query_obj_repr['ancestors'] and len(self._query_obj_repr['ancestors']) > 0:\n",
    "      self._ancestors = self._query_obj_repr['ancestors']\n",
    "    else:\n",
    "      self._ancestors = []\n",
    "\n",
    "  def get_parents(self) -> list:\n",
    "    return self._parents\n",
    "  \n",
    "  def get_ancestors(self) -> list:\n",
    "    return self._ancestors\n",
    "  \n",
    "  def get_all_subsumptive_targets(self) -> list:\n",
    "    return [self._target, *self._parents, *self._ancestors]\n",
    "  \n",
    "  def get_unique_subsumptive_targets(self) -> list:\n",
    "    return unique_unhashable_object_list(\n",
    "      self.get_all_subsumptive_targets()\n",
    "    )\n",
    "\n",
    "  def get_sorted_subsumptive_targets(self, key=\"depth\", reverse=False, depth_cutoff=3) -> list:\n",
    "    xs = self.get_all_subsumptive_targets()\n",
    "    xs.sort(key=lambda x: x[key], reverse=reverse)\n",
    "    return xs[:depth_cutoff]\n",
    "  \n",
    "  def get_unique_sorted_subsumptive_targets(self, key=\"depth\", reverse=False, depth_cutoff=3) -> list:\n",
    "    return unique_unhashable_object_list(\n",
    "      self.get_sorted_subsumptive_targets(key=key, reverse=reverse, depth_cutoff=depth_cutoff)\n",
    "    )\n",
    "  \n",
    "  def get_targets_with_dcg(self, type=\"exp\", depth_cutoff=3, **kwargs) -> tuple[float, list[dict]]:\n",
    "    # get targets (target, parents, ancestors) ordered in ascending via depth\n",
    "    targets_asc_depth = self.get_unique_sorted_subsumptive_targets(key=\"depth\", depth_cutoff=depth_cutoff)\n",
    "    # increase depth by 1 (offsetting zero-based index)\n",
    "    targets_w_offset = [\n",
    "      {**x, \"depth\": x[\"depth\"] + 1}\n",
    "      for x in targets_asc_depth\n",
    "    ]\n",
    "    # find the max depth (to calculate relevancy): (max_depth - depth_at_pos_k) + zero_based_offset\n",
    "    # which we refer to as: relevancy := ascent height + zero_based_offset\n",
    "    max_target_depth = reduce(\n",
    "      obj_max_depth, \n",
    "      targets_w_offset, \n",
    "      0\n",
    "    )\n",
    "    # calculate relevance for each target (node/parent/ancestor) \n",
    "    targets_with_rel = [\n",
    "      {**x, \"relevance\": (max_target_depth - x[\"depth\"]) + 1}\n",
    "      for x in targets_w_offset\n",
    "    ]\n",
    "    # ensure targets are sorted by relevance\n",
    "    targets_with_rel.sort(key=lambda x: x['relevance'], reverse=True)\n",
    "    # calculate dcg:\n",
    "    if type == \"linear\":\n",
    "      targets_with_dcg = [\n",
    "        {**x, \"dcg\": dcg_linear_relevancy_at_pos(x['relevance'], rank)}\n",
    "        for rank, x in enumerate(targets_with_rel, start=1)\n",
    "      ]\n",
    "    else:\n",
    "      targets_with_dcg = [\n",
    "        {**x, \"dcg\": dcg_exp_relevancy_at_pos(x['relevance'], rank)}\n",
    "        for rank, x in enumerate(targets_with_rel, start=1)\n",
    "      ]\n",
    "    iDCG = reduce(accumulate, targets_with_dcg, 0)\n",
    "    self._idcg = iDCG\n",
    "    self._targets_with_dcg = targets_with_dcg\n",
    "    return iDCG, targets_with_dcg\n",
    "\n",
    "  def get_ideal_dcg(self, type=\"exp\"):\n",
    "    if self._idcg:\n",
    "      return self._idcg\n",
    "    iDCG, targets = self.get_targets_with_dcg()\n",
    "    return iDCG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b4aa2c",
   "metadata": {},
   "source": [
    "**QueryObjectMapping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d158e502",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryObjectMapping:\n",
    "\n",
    "  _loaded: bool\n",
    "  _data_file_path: Path\n",
    "  _data: list\n",
    "  _equiv_queries: list\n",
    "  _subsumpt_queries: list\n",
    "\n",
    "  @validate_call\n",
    "  def __init__(self, json_data_fp: Path):\n",
    "    self._loaded = False\n",
    "    self._data_file_path = json_data_fp\n",
    "    self._load()\n",
    "    self._map()\n",
    "\n",
    "  def _load(self) -> None:\n",
    "    with self._data_file_path.open('r', encoding='utf-8') as fp:\n",
    "      self._data = json.load(fp)\n",
    "    self._loaded = True\n",
    "\n",
    "  @validate_call\n",
    "  def load_from_path(self, json_data_fp: Path) -> None:\n",
    "    # overwrite an existing file path\n",
    "    self._data_file_path = json_data_fp\n",
    "    self._load()\n",
    "\n",
    "  # equivalence_retrieval: bool = True, subsumption_retrieval: bool = False\n",
    "  def _map(self) -> None:\n",
    "    equiv_queries = []\n",
    "    subsumpt_queries = []\n",
    "    for query_obj_repr in self._data:\n",
    "      # if the query obj within the data contains an equiv class\n",
    "      if len(query_obj_repr['equivalent_classes']) > 0:\n",
    "        # we're dealing with an equiv query\n",
    "        equiv_queries.append(EquivQuery(query_obj_repr))\n",
    "      else:\n",
    "        subsumpt_queries.append(SubsumptionQuery(query_obj_repr))\n",
    "    self._equiv_queries = equiv_queries\n",
    "    self._subsumpt_queries = subsumpt_queries\n",
    "\n",
    "  def get_queries(self) -> tuple[list, list]:\n",
    "    return (self._equiv_queries, self._subsumpt_queries)\n",
    "  \n",
    "  def get_subsumpt_queries_with_no_transformations(self):\n",
    "    tmp_subsumpt_queries = copy.deepcopy(self._subsumpt_queries)\n",
    "    result_queries = []\n",
    "    for query in tmp_subsumpt_queries:\n",
    "      if query._entity_mention['transformed_entity_literal_for_type_alignment'] == \"\":\n",
    "        result_queries.append(query)\n",
    "    return result_queries\n",
    "  \n",
    "  def get_subsumpt_queries_with_transformations_only(self):\n",
    "    tmp_subsumpt_queries = copy.deepcopy(self._subsumpt_queries)\n",
    "    result_queries = []\n",
    "    for query in tmp_subsumpt_queries:\n",
    "      if query._entity_mention['transformed_entity_literal_for_type_alignment'] != \"\":\n",
    "        result_queries.append(query)\n",
    "    return result_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7819882a",
   "metadata": {},
   "source": [
    "**QueryResult**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7a60a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryResult(NamedTuple):\n",
    "  rank: int\n",
    "  iri: str\n",
    "  score: float\n",
    "  verbalisation: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648385c4",
   "metadata": {},
   "source": [
    "<style>\n",
    "*{\n",
    "    line-height: 24px;\n",
    "}\n",
    "</style>\n",
    "\n",
    "## Retrievers\n",
    "\n",
    "*Source: [retrievers.py](../src/hroov/utils/retrievers.py), also see: [gpu_retrievers.py](../src/hroov/utils/gpu_retrievers.py) for GPU accelerated retrieval.*\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57584d2e",
   "metadata": {},
   "source": [
    "**BaseRetriever**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cbce792",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseRetriever(ABC):\n",
    "    \n",
    "  _verbalisations: list\n",
    "  _meta_map: list\n",
    "\n",
    "  @validate_call\n",
    "  def __init__(self, verbalisations_fp: Path, meta_map_fp: Path):\n",
    "    with open(verbalisations_fp, 'r', encoding='utf-8') as fp:\n",
    "      self._verbalisations = json.load(fp)\n",
    "    with open(meta_map_fp, 'r', encoding='utf-8') as fp:\n",
    "       self._meta_map = json.load(fp)\n",
    "\n",
    "  @abstractmethod\n",
    "  def retrieve(self, query_string: str, *, top_k: int = 10, **kwargs) -> list[QueryResult]:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465618fe",
   "metadata": {},
   "source": [
    "**BaseModelRetriever**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "961f6bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModelRetriever(BaseRetriever):\n",
    "   \n",
    "  _embeddings: np.ndarray\n",
    "  _candidate_indicies: np.ndarray\n",
    "  _model: Union[SentenceTransformer, HierarchyTransformer, OntologyTransformer]\n",
    "  _score_fn: Callable\n",
    "\n",
    "  @overload\n",
    "  def __init__(self, verbalisations_fp: Path, meta_map_fp: Path, embeddings_fp: Path): ...\n",
    "\n",
    "  @overload\n",
    "  def __init__(self, verbalisations_fp: Path, meta_map_fp: Path, embeddings_fp: Path, *, score_fn: Callable | None = None): ...\n",
    "    \n",
    "  @overload\n",
    "  def __init__(self, verbalisations_fp: Path, meta_map_fp: Path, embeddings_fp: Path, *, score_fn: Callable | None = None, model_fp: Path | None = None): ...\n",
    "    \n",
    "  @overload\n",
    "  def __init__(self, verbalisations_fp: Path, meta_map_fp: Path, embeddings_fp: Path, *, score_fn: Callable | None = None, model_fp: Path | None = None, model_str: str | None = None): ...\n",
    "\n",
    "  @validate_call\n",
    "  def __init__(self, verbalisations_fp: Path, meta_map_fp: Path, embeddings_fp: Path, *, score_fn: Callable | None = None, model_fp: Path | None = None, model_str: str | None = None):\n",
    "    super().__init__(verbalisations_fp, meta_map_fp)\n",
    "    self._embeddings = np.load(embeddings_fp, mmap_mode=\"r\")\n",
    "    self._candidate_indicies = np.arange(len(self._embeddings))\n",
    "    if score_fn:\n",
    "      self.register_score_function(score_fn)\n",
    "    if model_fp:\n",
    "      try:\n",
    "        self.register_local_model(model_fp.expanduser().resolve())\n",
    "      except FileNotFoundError:\n",
    "        self.register_model(str(model_fp))\n",
    "    elif (not model_fp and model_str):\n",
    "      self.register_model(model_str)\n",
    "\n",
    "  def register_score_function(self, score_fn: Callable):\n",
    "    self._score_fn = score_fn\n",
    "\n",
    "  @override\n",
    "  def retrieve(self, query_string: str, *, top_k: int | None = None, reverse_candidate_scores=False, **kwargs) -> list[QueryResult]:\n",
    "    \"\"\"\n",
    "    TODO: 1. add docstring explaining why **kwargs is accepted and pass through to _score_fn\n",
    "          2. add explaination of parameters\n",
    "          3. types (args/return)\n",
    "    \"\"\"\n",
    "    query_embedding = self._embed(query_string)\n",
    "    scored_embeddings = self._score_fn(query_embedding, self._embeddings, **kwargs)\n",
    "    if reverse_candidate_scores and top_k is not None:\n",
    "      top_k_indicies = self._candidate_indicies[np.flip(np.argsort(scored_embeddings))[:top_k]]\n",
    "    elif not reverse_candidate_scores and top_k is not None:\n",
    "      top_k_indicies = self._candidate_indicies[np.argsort(scored_embeddings)[:top_k]]\n",
    "    elif reverse_candidate_scores and top_k is None:\n",
    "      top_k_indicies = self._candidate_indicies[np.flip(np.argsort(scored_embeddings))]\n",
    "    elif not reverse_candidate_scores and top_k is None:\n",
    "      top_k_indicies = self._candidate_indicies[np.argsort(scored_embeddings)]\n",
    "    else:\n",
    "      raise KeyError(\"Valid arguments for reverse_candidate_scores and top_k must be set.\")\n",
    "    results = []\n",
    "    for rank, candidate_index in enumerate(top_k_indicies):\n",
    "      candidate_score = scored_embeddings[candidate_index]\n",
    "      candidate_meta_map = self._meta_map[candidate_index]\n",
    "      candidate_verbalisation = candidate_meta_map['verbalisation']\n",
    "      candidate_iri = candidate_meta_map['iri']\n",
    "      results.append((rank, candidate_iri, candidate_score, candidate_verbalisation))\n",
    "    return results\n",
    "\n",
    "  @abstractmethod\n",
    "  def register_model(self, model: str) -> None:\n",
    "    pass\n",
    "\n",
    "  @abstractmethod\n",
    "  def register_local_model(self, model_fp: Path) -> None:\n",
    "    pass\n",
    "\n",
    "  @abstractmethod\n",
    "  def _embed(self, query_string: str) -> np.ndarray:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c635cf",
   "metadata": {},
   "source": [
    "**HiTRetriever**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e598af2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiTRetriever(BaseModelRetriever):\n",
    "\n",
    "  @override\n",
    "  def register_model(self, model: str) -> None:\n",
    "    self._model = HierarchyTransformer.from_pretrained(model)\n",
    "\n",
    "  @override\n",
    "  def register_local_model(self, model_fp: Path) -> None:\n",
    "    self._model = HierarchyTransformer.from_pretrained(str(model_fp.expanduser().resolve()))  \n",
    "\n",
    "  @override\n",
    "  def _embed(self, query_string: str) -> np.ndarray:\n",
    "    return (self._model.encode(\n",
    "      [query_string]\n",
    "    ).astype(\"float32\"))[0] # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1137c475",
   "metadata": {},
   "source": [
    "**OnTRetriever**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71096440",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnTRetriever(BaseModelRetriever):\n",
    "\n",
    "  @override\n",
    "  def register_model(self, model: str) -> None:\n",
    "    self._model = OntologyTransformer.load(model)\n",
    "\n",
    "  @override\n",
    "  def register_local_model(self, model_fp: Path) -> None:\n",
    "    self._model = OntologyTransformer.load(str(model_fp.expanduser().resolve()))\n",
    "\n",
    "  @override\n",
    "  def _embed(self, query_string: str) -> np.ndarray:\n",
    "    return (self._model.encode_concept(\n",
    "      [query_string]\n",
    "    ).astype(\"float32\"))[0] # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562da8e5",
   "metadata": {},
   "source": [
    "**SBERTRetriever**\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2879548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SBERTRetriever(BaseModelRetriever):\n",
    "\n",
    "  @override\n",
    "  def register_model(self, model: str) -> None:\n",
    "    self._model = SentenceTransformer.load(model)\n",
    "\n",
    "  @override\n",
    "  def register_local_model(self, model_fp: Path) -> None:\n",
    "    self._model = SentenceTransformer.load(str(model_fp.expanduser().resolve()))\n",
    "\n",
    "  @override\n",
    "  def _embed(self, query_string: str) -> np.ndarray:\n",
    "    return (self._model.encode(\n",
    "      [query_string]\n",
    "    ).astype(\"float32\"))[0] # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9032414",
   "metadata": {},
   "source": [
    "**BM25Retriever**\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c177a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25Retriever(BaseRetriever):\n",
    "    \n",
    "  _bm25: BM25Okapi\n",
    "\n",
    "  @validate_call\n",
    "  def __init__(self, verbalisations_fp: Path, meta_map_fp: Path, k1: float = 1.3, b: float = 0.7):\n",
    "    super().__init__(verbalisations_fp, meta_map_fp)\n",
    "    self._tokenised_verbalisations = parallel_tokenise(self._verbalisations, workers=4)\n",
    "    self._bm25 = BM25Okapi(self._tokenised_verbalisations, k1=k1, b=b)\n",
    "\n",
    "  @classmethod\n",
    "  def build_from_index(cls, index_fp: Path | str):\n",
    "    pass\n",
    "\n",
    "  def save_index(self, index_fp: Path | str):\n",
    "    if isinstance(index_fp, Path):\n",
    "      index_fp = str(index_fp.expanduser().resolve())\n",
    "    with open(index_fp, \"wb\") as fp:\n",
    "      pickle.dump({\n",
    "        \"index\": self._bm25,\n",
    "        \"verbalisations\": self._verbalisations,\n",
    "        \"meta_map\": self._meta_map\n",
    "      }, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"Saved BM25 index to disk.\")\n",
    "    \n",
    "  def load_index(self, index_fp: Path | str):\n",
    "    if isinstance(index_fp, Path):\n",
    "      index_fp = str(index_fp.expanduser().resolve())\n",
    "    with open(index_fp, \"rb\") as fp:\n",
    "      bm25_bin = pickle.load(fp)\n",
    "    self._bm25 = bm25_bin['index']\n",
    "    self._verbalisations = bm25_bin['verbalisations']\n",
    "    self._meta_map = bm25_bin['meta_map']\n",
    "\n",
    "  def retrieve(self, query_string: str, *, top_k: int | None = None, **kwargs) -> list[QueryResult]:\n",
    "    tokens = naive_tokenise(query_string)\n",
    "    scores = self._bm25.get_scores(tokens)\n",
    "    if top_k is not None:\n",
    "      top_idx = np.argsort(scores)[::-1][:top_k]\n",
    "    else:\n",
    "      top_idx = np.argsort(scores)[::-1]\n",
    "    results = []\n",
    "    for rank, idx in enumerate(top_idx):\n",
    "      iri = self._meta_map[idx]['iri']\n",
    "      verbalisation = self._verbalisations[idx]\n",
    "      results.append(\n",
    "        QueryResult(\n",
    "          rank=rank,\n",
    "          iri=iri,\n",
    "          score=float(scores[idx]),\n",
    "          verbalisation=verbalisation\n",
    "        )\n",
    "      )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7529347",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDFRetriever(BaseRetriever):\n",
    "  \n",
    "  # TODO: clean up and implement `save` and `load` methods, similar to BM25.\n",
    "\n",
    "  _vectorizer: TfidfVectorizer\n",
    "  _inverted_index: dict[str, list[tuple[int, float]]]\n",
    "  _tfidf_matrix: \"scipy.sparse.csr_matrix\"\n",
    "  _tokenizer: Callable[[str], Sequence[str]] | None\n",
    "\n",
    "  @validate_call\n",
    "  def __init__(self, verbalisations_fp: Path, meta_map_fp: Path, *,\n",
    "    lowercase: bool = True, stop_words: str | None = \"english\",\n",
    "    ngram_range: tuple[int, int] = (1, 1),\n",
    "    tokenizer: Callable[[str], Sequence[str]] | None = None,\n",
    "    max_features: int | None = None,\n",
    "  ) -> None:\n",
    "    super().__init__(verbalisations_fp, meta_map_fp)\n",
    "    self._vectorizer = TfidfVectorizer(\n",
    "      stop_words=\"english\",\n",
    "      use_idf=True,\n",
    "      smooth_idf=True,\n",
    "      # norm=\"l2\"\n",
    "      norm=None\n",
    "    )\n",
    "    doc_term_matrix = self._vectorizer.fit_transform(self._verbalisations)\n",
    "    vocab = self._vectorizer.get_feature_names_out()\n",
    "    inverted_index: dict[str, list[tuple[int, float]]] = {term: [] for term in vocab}\n",
    "    coo = coo_matrix(doc_term_matrix)\n",
    "\n",
    "    for row, col, score in zip(coo.row, coo.col, coo.data):\n",
    "      inverted_index[str(vocab[col])].append((int(row), float(score)))\n",
    "    for postings in inverted_index.values():\n",
    "      postings.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    self._inverted_index = inverted_index\n",
    "\n",
    "  def retrieve(self, query_string: str, *, top_k: int | None = None, **kwargs) -> list[QueryResult]:\n",
    "    query_vec = self._vectorizer.transform([query_string])\n",
    "    vocab = self._vectorizer.get_feature_names_out()        \n",
    "    q_weights = {\n",
    "      vocab[col]: float(val)\n",
    "      for col, val in zip(query_vec.indices, query_vec.data) # type: ignore\n",
    "      if val > 0.0\n",
    "    }\n",
    "    tfidf_scores = aggregate_posting_scores(q_weights, self._inverted_index)\n",
    "    if top_k:\n",
    "      tfidf_top = topk(tfidf_scores, top_k)\n",
    "    else:\n",
    "      tfidf_top = topk(tfidf_scores, len(tfidf_scores))\n",
    "    results: list[QueryResult] = []\n",
    "    for rank, (doc_id, score) in enumerate(tfidf_top, 1):\n",
    "      iri = self._meta_map[doc_id]['iri']\n",
    "      verbalisation = self._meta_map[doc_id]['verbalisation']\n",
    "      results.append(\n",
    "        QueryResult(\n",
    "          rank=rank,\n",
    "          iri=iri,\n",
    "          score=float(score),\n",
    "          verbalisation=verbalisation,\n",
    "        )\n",
    "      )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe86b0d6",
   "metadata": {},
   "source": [
    "<style>\n",
    "*{\n",
    "    line-height: 24px;\n",
    "}\n",
    "</style>\n",
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "Most evaluation metrics are actually defined within the context of the experiments themselves; i.e. they're not pre-defined as functions prior to implementation. However, some intuition for nDCG is provided below.\n",
    "\n",
    "Other evaluation metrics include:\n",
    "\n",
    "Hit rate / h@k, MRR, MR, Median Rank, PR-AUC, mAP and Recall@k.\n",
    "\n",
    "TODO: update.\n",
    "TODO: convert nDCG implementation details from python comments to something a little more palletable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db2f8bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nDCG NOTES:\n",
    "\n",
    "# We don't want to directly modify the depth (as that might be confusing)\n",
    "# So.. we're going to create a new set of subsumptive targets with a relevancy score\n",
    "# where the relevancy := ascent_height + 1 \\forall t \\in T\n",
    "# where the ascent_height := max(depth) - depth_at_t\n",
    "# -> relevancy := (max(depth) - depth_at_t) + 1 \\forall t \\in T\n",
    "#\n",
    "# Example (POLYHIERARCHICAL ONTOLOGY):\n",
    "#\n",
    "# Say, you've got a query_string with a target entity, two parent entities and seven ancestors \n",
    "# (exclusive of SNOMED CT CONCEPT & owl:Thing):\n",
    "# \n",
    "#                              ---------------------------------------------------------------------------------\n",
    "#                  ENTITY_TYPE | DEPTH | ASCENT_HEIGHT | ASCENT_HEIGHT + 1 (REL) | 2^{r} - 1 |  = val  |  DCG  |\n",
    "#                              |-------|---------------|-------------------------|-----------|---------|-------|\n",
    "#      X      <- TARGET_ENTITY |   0   |       4       |            5            |  2^5 - 1  |    31   |  31.0 |  \n",
    "#     / \\                      |       |               |                         |           |         |       |\n",
    "#    X   X          <- PARENTS |   1   |       3       |            4            |  2^4 - 1  |    15   |  9.46 |\n",
    "#   /   / \\                    |       |               |                         |           |         |       |\n",
    "#  X   X   X      <- ANCESTORS |   2   |       2       |            3            |  2^3 - 1  |    7    |  7.50 |\n",
    "#  |   |    \\                  |       |               |                         |           |         |       |\n",
    "#  |   X     X    <- ANCESTORS |   3   |       1       |            2            |  2^2 - 1  |    3    |  3.57 |\n",
    "#  |   |    / \\                |       |               |                         |           |         |       |\n",
    "#  |   |   X   X  <- ANCESTORS |   4   |       0       |            1            |  2^1 - 1  |    1    |  1.61 |\n",
    "#  |   |   |   |               ---------------------------------------------------------------------------------\n",
    "#  -------------------------                     \n",
    "#  | TOP SNOMED CT CONCEPT |\n",
    "#  -------------------------\n",
    "#  |       owl:Thing       |\n",
    "#  -------------------------------------------------------------------------------------------------------\n",
    "#  |              IDEAL ORDERING                   |                  EXAMPLE RESULTS                    |\n",
    "#  -------------------------------------------------------------------------------------------------------\n",
    "#  |  RANK      ENTITY NAME          REL    DCG    |  RANK      ENTITY               REL     DCG         |\n",
    "#  -------------------------------------------------------------------------------------------------------\n",
    "#  |  1         TARGET_ENTITY        5      31.0   |  1         ANCESTOR_@_3_1|2     2       3.00        |\n",
    "#  |  2         PARENT_1|2           4      9.46   |  2         ANCESTOR_@_2_1|2|3   3       4.42        |\n",
    "#  |  3         PARENT_2|2           4      7.50   |  3         TARGET_ENTITY        5       15.5        |\n",
    "#  |  4         ANCESTOR_@_2_1|2|3   3      3.01   |  4         PARENT_1|2           4       6.46        |\n",
    "#  |  5         ANCESTOR_@_2_1|2|3   3      2.71   |  5         ANCESTOR_@_2_1|2|3   3       2.71        |\n",
    "#  |  6         ANCESTOR_@_2_1|2|3   3      2.49   |  6         ANCESTOR_@_2_1|2|3   3       2.49        |\n",
    "#  |  7         ANCESTOR_@_3_1|2     2      1.00   |  7         ANCESTOR_@_3_1|2     2       1.00        |\n",
    "#  |  8         ANCESTOR_@_3_1|2     2      0.95   |  8         NOT-RELEVANT-RESULT  0       0.00        |\n",
    "#  |  9         ANCESTOR_@_4_1|2     1      0.30   |  9         ANCESTOR_@_4_1|2     1       0.30        |\n",
    "#  |  10        ANCESTOR_@_4_1|2     1      0.29   |  10        NOT-RELEVANT-RESULT  0       0.00        |\n",
    "# --------------------------------------------------------------------------------------------------------\n",
    "#  |     iDCG = \\sum_i^{\\|T\\|}t_{dcg} = 58.72      |  DCG@10 = \\sum_i^{\\|Q_{results}\\|}q_dcg = 35.88     |\n",
    "# --------------------------------------------------------------------------------------------------------\n",
    "#\n",
    "#     nDCG@10 =  DCG@10        35.88\n",
    "#               --------   =   -----   =  0.611\n",
    "#                iDCG@10       58.72\n",
    "#\n",
    "#\n",
    "#  * depth is measured from the target.\n",
    "#  * Quick Recap: we're taking an OOV phrase/string (from a set of entity mentions on a QA dataset)\n",
    "#    -> assigning a target SNOMED entity to that entity mention, s.t. SNOMED entity ~= entity mention\n",
    "#    -> for the entity mention, we traverse the ontology from the target, up the hierarchy, until we reach the top\n",
    "#    -> as we traverse the structure, we record the depth, rdfs:label, pref:label alt:labels and IRI of each node\n",
    "#    -> that allows us to construct a graph (that kind of looks like a tree, as its a fragment of an ontology, which is\n",
    "#       largely a taxonomy, but it is polyhierarchical, so it ends up being a graph)\n",
    "#    -> as we get further away from the target, the concepts get more general/broad, so assume/consider the relevancy\n",
    "#       decreases monotonically as a function of the depth (we implement two relevancy scores for DCG:\n",
    "#           \n",
    "#           (1) Relevancy \\w exponential decay:  \\frac{2^{rel} - 1}{log_2(rank+1)}\n",
    "#           (2) Relevancy \\w linear decay: \\frac{rel}{ln(rank+1)}\n",
    "#\n",
    "#    -> We opt to use exponential decay, as it more suitably approximates distance in hyperbolic space, though, it is noted\n",
    "#       that the result is normalised anyway..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "114a4e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from typing import Any\n",
    "import math\n",
    "\n",
    "def add(a, b, key='dcg'):\n",
    "  return a + b[key]\n",
    "\n",
    "def obj_max_depth(x: int, obj: Any, key: str = \"depth\") -> int:\n",
    "  return x if x > obj[key] else obj[key]\n",
    "\n",
    "def dcg_exp_relevancy_at_pos(relevancy: int, rank_position: int) -> float:\n",
    "  if relevancy <= 0:\n",
    "    return float(0.0)\n",
    "  numerator = (2**relevancy) - 1\n",
    "  denominator = math.log2(rank_position + 1)\n",
    "  return float(numerator / denominator)\n",
    "\n",
    "def compute_ndcg_at_k(results: list[tuple[int, str, float, str]], targets_with_dcg_exp: list[dict], k: int = 20) -> float:\n",
    "  relevance_map = {target['iri']: target['relevance'] for target in targets_with_dcg_exp}\n",
    "  dcg = 0.0\n",
    "  for rank, (idx, iri, score, label) in enumerate(results[:k], start=1):\n",
    "    rel = relevance_map.get(iri, 0)\n",
    "    dcg += dcg_exp_relevancy_at_pos(rel, rank)\n",
    "  ideal_dcg = sum(target['dcg'] for target in targets_with_dcg_exp[:k])\n",
    "  if ideal_dcg == 0:\n",
    "    return 0.0\n",
    "  \n",
    "  return dcg / ideal_dcg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc31936d",
   "metadata": {},
   "source": [
    "<style>\n",
    "*{\n",
    "    line-height: 24px;\n",
    "}\n",
    "</style>\n",
    "\n",
    "## Embedding & Retrieval \n",
    "\n",
    "#### Precompute Embeddings\n",
    "\n",
    "1. Process the entity lexicon (or $\\mathcal{EL}$-normalised concept strings) and extract a verbalisation list.\n",
    "2. Encode each concepts' textual representation into each models native embedding space.\n",
    "3. Save the embeddings to disk, along with a map between $emb \\leftrightarrow concept_{text\\ repr}$.\n",
    "\n",
    "#### Retrieval & Ranking\n",
    "\n",
    "1. Load the embeddings + their mappings.\n",
    "2. Accept a query string $q$.\n",
    "3. Compute a score for $q$ using a retrieval method: $\\{ TFIDF, BM25, SBERT, HiT, OnT \\}$ using the pre-computed embeddings.\n",
    "4. `argsort` the embs according to their scores\n",
    "5. Return the sorted list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7255409d",
   "metadata": {},
   "source": [
    "**Embedding:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65e14f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for indexing/encoding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375724/375724 [00:01<00:00, 344608.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete!\n"
     ]
    }
   ],
   "source": [
    "# prep for embeding:\n",
    "\n",
    "print(\"Preparing data for indexing/encoding...\")\n",
    "\n",
    "data_dir = \"../data\"\n",
    "embeddings_dir = \"../embeddings\"\n",
    "\n",
    "if not (Path(data_dir).exists()):\n",
    "  print(\"[WARNING] No data directory exists. The notebook will fail. Review the README.md, or the docs dir.\")\n",
    "\n",
    "# if an embeddings dir has not yet been created, create one    \n",
    "Path(embeddings_dir).expanduser().resolve().mkdir(parents=True, exist_ok=True)\n",
    "\n",
    " # generated during SNOMED CT processing\n",
    "entity_lexicon_fp = Path(f\"{data_dir}/preprocessed_entity_lexicon.json\")\n",
    "\n",
    "# list of the verbalisations (label text, or deeponto verbs)\n",
    "verbalisation_list_fp = Path(f\"{embeddings_dir}/verbalisations.json\")\n",
    "# each index of the entity_map points to a tuple: (index, label, verbalisation, iri)\n",
    "entity_map_fp = Path(f\"{embeddings_dir}/entity_map.json\")\n",
    "# compiles a list of the above mappings (handy when it comes to argsort)\n",
    "entity_mappings_list_fp = Path(f\"{embeddings_dir}/entity_mappings.json\")\n",
    "\n",
    "entity_lexicon = load_json(entity_lexicon_fp)\n",
    "iris = entity_lexicon.keys()\n",
    "\n",
    "entity_map = {}\n",
    "entity_verbalisation_list = []\n",
    "list_of_entity_mappings = []\n",
    "\n",
    "for entity_idx, entity_iri in enumerate(tqdm(iris)):\n",
    "    entity_map[str(entity_idx)] = {\n",
    "        \"mapping_id\": str(entity_idx),\n",
    "        \"label\": entity_lexicon[entity_iri].get('name'), # type: ignore\n",
    "        \"verbalisation\": strip_parens(str(entity_lexicon[entity_iri].get('name'))).lower(), # type: ignore\n",
    "        \"iri\": entity_iri\n",
    "    }\n",
    "    entity_verbalisation_list.append(strip_parens(str(entity_lexicon[entity_iri].get('name'))).lower()) # type: ignore\n",
    "    list_of_entity_mappings.append(entity_map[str(entity_idx)])\n",
    "\n",
    "save_json(verbalisation_list_fp, entity_verbalisation_list)\n",
    "save_json(entity_map_fp, entity_map)\n",
    "save_json(entity_mappings_list_fp, list_of_entity_mappings)\n",
    "\n",
    "print(\"Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56d06f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_already_exist = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "049c0917",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not embs_already_exist:\n",
    "\n",
    "  sbert_plm_hf_string = \"all-MiniLM-L12-v2\"\n",
    "  sbert_plm_encoder = SentenceTransformer.load(sbert_plm_hf_string)\n",
    "  sbert_plm_embeddings = sbert_plm_encoder.encode(\n",
    "      entity_verbalisation_list,\n",
    "      batch_size=128,\n",
    "      show_progress_bar=True,\n",
    "      normalize_embeddings=True\n",
    "  ).astype(\"float32\")\n",
    "  np.save(f\"{embeddings_dir}/sbert-plm-embeddings.npy\", sbert_plm_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ba864f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not embs_already_exist:\n",
    "\n",
    "  hit_snomed_25_model_fp = '../models/snomed_models/HiT-mixed-SNOMED-25/final'\n",
    "  hit_snomed_25_encoder = HierarchyTransformer.from_pretrained(hit_snomed_25_model_fp)\n",
    "  hit_snomed_25_embeddings = hit_snomed_25_encoder.encode(\n",
    "      entity_verbalisation_list,\n",
    "      batch_size=128,\n",
    "      show_progress_bar=True\n",
    "  ).astype(\"float32\")\n",
    "  np.save(f\"{embeddings_dir}/hit-snomed-25-embeddings.npy\", hit_snomed_25_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47c91545",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not embs_already_exist:\n",
    "\n",
    "  ont_galen_23_pred_model_fp = \"../models/models/prediction/OnTr-all-MiniLM-L12-v2-GALEN\"\n",
    "  ont_galen_23_pred_encoder = OntologyTransformer.load(ont_galen_23_pred_model_fp)\n",
    "  ont_galen_23_pred_embeddings = ont_galen_23_pred_encoder.encode_concept(\n",
    "      entity_verbalisation_list,\n",
    "      batch_size=128,\n",
    "      show_progress_bar=True\n",
    "  ).astype(\"float32\")\n",
    "  np.save(f\"{embeddings_dir}/ont-galen-23-pred-embeddings.npy\", ont_galen_23_pred_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0f10998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not embs_already_exist:\n",
    "\n",
    "  ont_anatomy_23_pred_model_fp = \"../models/models/prediction/OnTr-all-MiniLM-L12-v2-ANATOMY\"\n",
    "  ont_anatomy_23_pred_encoder = OntologyTransformer.load(ont_anatomy_23_pred_model_fp)\n",
    "  ont_anatomy_23_pred_embeddings = ont_anatomy_23_pred_encoder.encode_concept(\n",
    "      entity_verbalisation_list,\n",
    "      batch_size=128,\n",
    "      show_progress_bar=True\n",
    "  ).astype(\"float32\")\n",
    "  np.save(f\"{embeddings_dir}/ont-anatomy-23-pred-embeddings.npy\", ont_anatomy_23_pred_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e52e54d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not embs_already_exist:\n",
    "\n",
    "  ont_gene_ontology_23_pred_model_fp = \"../models/models/prediction/OnTr-all-MiniLM-L12-v2-GO\"\n",
    "  ont_gene_ontology_23_pred_encoder = OntologyTransformer.load(ont_gene_ontology_23_pred_model_fp)\n",
    "  ont_gene_ontology_23_pred_embeddings = ont_gene_ontology_23_pred_encoder.encode_concept(\n",
    "      entity_verbalisation_list,\n",
    "      batch_size=128,\n",
    "      show_progress_bar=True\n",
    "  ).astype(\"float32\")\n",
    "  np.save(f\"{embeddings_dir}/ont-go-23-pred-embeddings.npy\", ont_gene_ontology_23_pred_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a703f3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not embs_already_exist:\n",
    "\n",
    "  ont_snomed_LG_model_fp = '../models/snomed_models/OnT-LG'\n",
    "  ont_snomed_LG_encoder = OntologyTransformer.load(ont_snomed_LG_model_fp)\n",
    "  ont_snomed_LG_embeddings = ont_snomed_LG_encoder.encode_concept(\n",
    "      entity_verbalisation_list,\n",
    "      batch_size=128,\n",
    "      show_progress_bar=True\n",
    "  ).astype(\"float32\")\n",
    "  np.save(f\"{embeddings_dir}/ont-snomed-LG-embeddings.npy\", ont_snomed_LG_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8e100815",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not embs_already_exist:\n",
    "\n",
    "  ont_snomed_minified_32_model_fp = '../models/snomed_models/OnTr-m-32'\n",
    "  ont_snomed_m_32_encoder = OntologyTransformer.load(ont_snomed_minified_32_model_fp)\n",
    "  ont_snomed_minified_32_embeddings = ont_snomed_m_32_encoder.encode_concept(\n",
    "      entity_verbalisation_list,\n",
    "      batch_size=128,\n",
    "      show_progress_bar=True\n",
    "  ).astype(\"float32\")\n",
    "  np.save(f\"{embeddings_dir}/ont-snomed-minified-32-embeddings.npy\", ont_snomed_minified_32_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "14e8508b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not embs_already_exist:\n",
    "\n",
    "  ont_snomed_minified_model_fp = '../models/snomed_models/OnTr-minified-64'\n",
    "  ont_snomed_encoder = OntologyTransformer.load(ont_snomed_minified_model_fp)\n",
    "  ont_snomed_minified_embeddings = ont_snomed_encoder.encode_concept(\n",
    "      entity_verbalisation_list,\n",
    "      batch_size=128,\n",
    "      show_progress_bar=True\n",
    "  ).astype(\"float32\")\n",
    "  np.save(f\"{embeddings_dir}/ont-snomed-minified-embeddings.npy\", ont_snomed_minified_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0416e1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not embs_already_exist:\n",
    "    \n",
    "  ont_snomed_minified_128_model_fp = '../models/snomed_models/OnTr-m-128'\n",
    "  ont_snomed_m_128_encoder = OntologyTransformer.load(ont_snomed_minified_128_model_fp)\n",
    "  ont_snomed_minified_128_embeddings = ont_snomed_m_128_encoder.encode_concept(\n",
    "      entity_verbalisation_list,\n",
    "      batch_size=128,\n",
    "      show_progress_bar=True\n",
    "  ).astype(\"float32\")\n",
    "  np.save(f\"{embeddings_dir}/ont-snomed-minified-128-embeddings.npy\", ont_snomed_minified_128_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858ac779",
   "metadata": {},
   "source": [
    "**Retrieval**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccb7e75",
   "metadata": {},
   "source": [
    "*Load the pre-computed embeddings.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f81f43d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_plm_embs = np.load(f\"{embeddings_dir}/sbert-plm-embeddings.npy\", mmap_mode=\"r\")\n",
    "hit_snomed_25_embs = np.load(f\"{embeddings_dir}/hit-snomed-25-embeddings.npy\", mmap_mode=\"r\") # HiT FULL\n",
    "ont_galen_23_pred_embs = np.load(f\"{embeddings_dir}/ont-galen-23-pred-embeddings.npy\", mmap_mode=\"r\")\n",
    "ont_anatomy_23_pred_embs = np.load(f\"{embeddings_dir}/ont-anatomy-23-pred-embeddings.npy\", mmap_mode=\"r\")\n",
    "ont_gene_ontology_23_pred_embs = np.load(f\"{embeddings_dir}/ont-go-23-pred-embeddings.npy\", mmap_mode=\"r\")\n",
    "ont_snomed_LG_embs = np.load(f\"{embeddings_dir}/ont-snomed-LG-embeddings.npy\", mmap_mode=\"r\") # SNOMED FULL\n",
    "ont_minified_32_embs = np.load(f\"{embeddings_dir}/ont-snomed-minified-32-embeddings.npy\", mmap_mode=\"r\")\n",
    "ont_minified_embs = np.load(f\"{embeddings_dir}/ont-snomed-minified-embeddings.npy\", mmap_mode=\"r\") # M-64\n",
    "ont_minified_128_embs = np.load(f\"{embeddings_dir}/ont-snomed-minified-128-embeddings.npy\", mmap_mode=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c5a41a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dir = \"../embeddings\"\n",
    "common_map = Path(f\"{embeddings_dir}/entity_mappings.json\")\n",
    "common_verbalisations = Path(f\"{embeddings_dir}/verbalisations.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebc8ca3",
   "metadata": {},
   "source": [
    "**Retrievers: Lexical Methods**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eabe1562",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_ret = TFIDFRetriever(common_verbalisations, common_map)\n",
    "bm25_ret = BM25Retriever(common_verbalisations, common_map, k1=1.3, b=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10136238",
   "metadata": {},
   "source": [
    "**Retrievers: SBERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4e3eaba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_plm_hf_string = \"all-MiniLM-L12-v2\"\n",
    "\n",
    "sbert_ret_plm_w_cosine_sim = SBERTRetriever(\n",
    "  embeddings_fp=Path(f\"{embeddings_dir}/sbert-plm-embeddings.npy\"),\n",
    "  meta_map_fp=common_map,\n",
    "  verbalisations_fp=common_verbalisations,\n",
    "  model_str=\"all-MiniLM-L12-v2\",\n",
    "  score_fn=batch_cosine_similarity\n",
    ")\n",
    "\n",
    "sbert_ret_plm_w_euclid_dist = SBERTRetriever(\n",
    "  embeddings_fp=Path(f\"{embeddings_dir}/sbert-plm-embeddings.npy\"),\n",
    "  meta_map_fp=common_map,\n",
    "  verbalisations_fp=common_verbalisations,\n",
    "  model_str=\"all-MiniLM-L12-v2\",\n",
    "  score_fn=batch_euclidian_l2_distance\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f31bc7",
   "metadata": {},
   "source": [
    "**Retrievers: HiT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3946c053",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are trying to use a model that was created with Sentence Transformers version 5.0.0, but you're currently using version 4.1.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "WARNING:sentence_transformers.SentenceTransformer:You are trying to use a model that was created with Sentence Transformers version 5.0.0, but you're currently using version 4.1.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n"
     ]
    }
   ],
   "source": [
    "# Hierarchy Transformer-based Retriever (HiT Full)\n",
    "\n",
    "hit_snomed_25_model_fp = '../models/snomed_models/HiT-mixed-SNOMED-25/final'\n",
    "hit_SNOMED25_model_path = Path(hit_snomed_25_model_fp)\n",
    "\n",
    "hit_ret_snomed_25_w_hyp_dist = HiTRetriever(\n",
    "  embeddings_fp=Path(f\"{embeddings_dir}/hit-snomed-25-embeddings.npy\"),\n",
    "  meta_map_fp=common_map,\n",
    "  verbalisations_fp=common_verbalisations,\n",
    "  model_fp=hit_SNOMED25_model_path,\n",
    "  score_fn=batch_poincare_dist_with_adaptive_curv_k\n",
    ")\n",
    "\n",
    "hit_ret_snomed_25_w_ent_sub = HiTRetriever(\n",
    "  embeddings_fp=Path(f\"{embeddings_dir}/hit-snomed-25-embeddings.npy\"),\n",
    "  meta_map_fp=common_map,\n",
    "  verbalisations_fp=common_verbalisations,\n",
    "  model_fp=hit_SNOMED25_model_path,\n",
    "  score_fn=entity_subsumption\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77280d51",
   "metadata": {},
   "source": [
    "**Retrievers OnT:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718f04fb",
   "metadata": {},
   "source": [
    "#### OnT Encoders (ANATOMY, GALEN, GO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "98c7e518",
   "metadata": {},
   "outputs": [],
   "source": [
    "ont_anatomy_23_pred_model_fp = \"../models/models/prediction/OnTr-all-MiniLM-L12-v2-ANATOMY\"\n",
    "ont_anatonmy_pred_model_path = Path(ont_anatomy_23_pred_model_fp)\n",
    "\n",
    "ont_ret_anatomy_pred_w_hyp_dist = OnTRetriever(\n",
    "  embeddings_fp=Path(f\"{embeddings_dir}/ont-anatomy-23-pred-embeddings.npy\"),\n",
    "  meta_map_fp=common_map,\n",
    "  verbalisations_fp=common_verbalisations,\n",
    "  model_fp=ont_anatonmy_pred_model_path,\n",
    "  score_fn=batch_poincare_dist_with_adaptive_curv_k\n",
    ")\n",
    "\n",
    "ont_ret_anatomy_pred_w_con_sub = OnTRetriever(\n",
    "  embeddings_fp=Path(f\"{embeddings_dir}/ont-anatomy-23-pred-embeddings.npy\"),\n",
    "  meta_map_fp=common_map,\n",
    "  verbalisations_fp=common_verbalisations,\n",
    "  model_fp=ont_anatonmy_pred_model_path,\n",
    "  score_fn=concept_subsumption\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "26286b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ont_galen_23_pred_model_fp = \"../models/models/prediction/OnTr-all-MiniLM-L12-v2-GALEN\"\n",
    "ont_galen_pred_model_path = Path(ont_galen_23_pred_model_fp)\n",
    "\n",
    "ont_ret_galen_pred_w_hyp_dist = OnTRetriever(\n",
    "  embeddings_fp=Path(f\"{embeddings_dir}/ont-galen-23-pred-embeddings.npy\"),\n",
    "  meta_map_fp=common_map,\n",
    "  verbalisations_fp=common_verbalisations,\n",
    "  model_fp=ont_galen_pred_model_path,\n",
    "  score_fn=batch_poincare_dist_with_adaptive_curv_k\n",
    ")\n",
    "\n",
    "ont_ret_galen_pred_w_con_sub = OnTRetriever(\n",
    "  embeddings_fp=Path(f\"{embeddings_dir}/ont-galen-23-pred-embeddings.npy\"),\n",
    "  meta_map_fp=common_map,\n",
    "  verbalisations_fp=common_verbalisations,\n",
    "  model_fp=ont_galen_pred_model_path,\n",
    "  score_fn=concept_subsumption\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "022c14c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ont_gene_ontology_23_pred_model_fp = \"../models/models/prediction/OnTr-all-MiniLM-L12-v2-GO\"\n",
    "ont_go_pred_model_path = Path(ont_gene_ontology_23_pred_model_fp)\n",
    "\n",
    "ont_ret_go_pred_w_hyp_dist = OnTRetriever(\n",
    "    embeddings_fp=Path(f\"{embeddings_dir}/ont-go-23-pred-embeddings.npy\"),\n",
    "    meta_map_fp=common_map,\n",
    "    verbalisations_fp=common_verbalisations,\n",
    "    model_fp=ont_go_pred_model_path,\n",
    "    score_fn=batch_poincare_dist_with_adaptive_curv_k\n",
    ")\n",
    "\n",
    "ont_ret_go_pred_w_con_sub = OnTRetriever(\n",
    "    embeddings_fp=Path(f\"{embeddings_dir}/ont-go-23-pred-embeddings.npy\"),\n",
    "    meta_map_fp=common_map,\n",
    "    verbalisations_fp=common_verbalisations,\n",
    "    model_fp=ont_go_pred_model_path,\n",
    "    score_fn=concept_subsumption\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec09246e",
   "metadata": {},
   "source": [
    "#### OnT SNOMED-CT Tuned Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "88a86c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "ont_snomed_LG_model_fp = \"../models/snomed_models/OnT-LG\"\n",
    "ont_snomed_LG_model_path = Path(ont_snomed_LG_model_fp)\n",
    "\n",
    "ont_snomed_LG_w_hyp_dist = OnTRetriever(\n",
    "    embeddings_fp=Path(f\"{embeddings_dir}/ont-snomed-LG-embeddings.npy\"),\n",
    "    meta_map_fp=common_map,\n",
    "    verbalisations_fp=common_verbalisations,\n",
    "    model_fp=ont_snomed_LG_model_path,\n",
    "    score_fn=batch_poincare_dist_with_adaptive_curv_k\n",
    ")\n",
    "\n",
    "ont_snomed_LG_w_con_sub = OnTRetriever(\n",
    "    embeddings_fp=Path(f\"{embeddings_dir}/ont-snomed-LG-embeddings.npy\"),\n",
    "    meta_map_fp=common_map,\n",
    "    verbalisations_fp=common_verbalisations,\n",
    "    model_fp=ont_snomed_LG_model_path,\n",
    "    score_fn=concept_subsumption\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "38ed168c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sentence_transformers.SentenceTransformer:You are trying to use a model that was created with Sentence Transformers version 5.0.0, but you're currently using version 4.1.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "WARNING:sentence_transformers.SentenceTransformer:You are trying to use a model that was created with Sentence Transformers version 5.0.0, but you're currently using version 4.1.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n"
     ]
    }
   ],
   "source": [
    "ontr_snomed_minified_32_model_fp = '../models/snomed_models/OnTr-m-32'\n",
    "ontr_snomed_minified_32_model_path = Path(ontr_snomed_minified_32_model_fp)\n",
    "\n",
    "ontr_ret_snomed_minified_32_w_hyp_dist = OnTRetriever(\n",
    "    embeddings_fp=Path(f\"{embeddings_dir}/ont-snomed-minified-32-embeddings.npy\"),\n",
    "    meta_map_fp=common_map,\n",
    "    verbalisations_fp=common_verbalisations,\n",
    "    model_fp=ontr_snomed_minified_32_model_path,\n",
    "    score_fn=batch_poincare_dist_with_adaptive_curv_k\n",
    ")\n",
    "\n",
    "ontr_ret_snomed_minified_32_w_con_sub = OnTRetriever(\n",
    "    embeddings_fp=Path(f\"{embeddings_dir}/ont-snomed-minified-32-embeddings.npy\"),\n",
    "    meta_map_fp=common_map,\n",
    "    verbalisations_fp=common_verbalisations,\n",
    "    model_fp=ontr_snomed_minified_32_model_path,\n",
    "    score_fn=concept_subsumption\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7dacae75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sentence_transformers.SentenceTransformer:You are trying to use a model that was created with Sentence Transformers version 5.0.0, but you're currently using version 4.1.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "WARNING:sentence_transformers.SentenceTransformer:You are trying to use a model that was created with Sentence Transformers version 5.0.0, but you're currently using version 4.1.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n"
     ]
    }
   ],
   "source": [
    "ontr_snomed_minified_model_fp = '../models/snomed_models/OnTr-minified-64'\n",
    "ontr_snomed_minified_model_fp = Path(ontr_snomed_minified_model_fp)\n",
    "\n",
    "ontr_ret_snomed_minified_w_hyp_dist = OnTRetriever(\n",
    "    embeddings_fp=Path(f\"{embeddings_dir}/ont-snomed-minified-embeddings.npy\"),\n",
    "    meta_map_fp=common_map,\n",
    "    verbalisations_fp=common_verbalisations,\n",
    "    model_fp=ontr_snomed_minified_model_fp,\n",
    "    score_fn=batch_poincare_dist_with_adaptive_curv_k\n",
    ")\n",
    "\n",
    "ontr_ret_snomed_minified_w_con_sub = OnTRetriever(\n",
    "    embeddings_fp=Path(f\"{embeddings_dir}/ont-snomed-minified-embeddings.npy\"),\n",
    "    meta_map_fp=common_map,\n",
    "    verbalisations_fp=common_verbalisations,\n",
    "    model_fp=ontr_snomed_minified_model_fp,\n",
    "    score_fn=concept_subsumption\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3ca3318a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sentence_transformers.SentenceTransformer:You are trying to use a model that was created with Sentence Transformers version 5.0.0, but you're currently using version 4.1.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "WARNING:sentence_transformers.SentenceTransformer:You are trying to use a model that was created with Sentence Transformers version 5.0.0, but you're currently using version 4.1.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n"
     ]
    }
   ],
   "source": [
    "ontr_snomed_minified_128_model_fp = '../models/snomed_models/OnTr-m-128'\n",
    "ontr_snomed_minified_128_model_fp = Path(ontr_snomed_minified_128_model_fp)\n",
    "\n",
    "ontr_ret_snomed_minified_128_w_hyp_dist = OnTRetriever(\n",
    "    embeddings_fp=Path(f\"{embeddings_dir}/ont-snomed-minified-128-embeddings.npy\"),\n",
    "    meta_map_fp=common_map,\n",
    "    verbalisations_fp=common_verbalisations,\n",
    "    model_fp=ontr_snomed_minified_128_model_fp,\n",
    "    score_fn=batch_poincare_dist_with_adaptive_curv_k\n",
    ")\n",
    "\n",
    "ontr_ret_snomed_minified_128_w_con_sub = OnTRetriever(\n",
    "    embeddings_fp=Path(f\"{embeddings_dir}/ont-snomed-minified-128-embeddings.npy\"),\n",
    "    meta_map_fp=common_map,\n",
    "    verbalisations_fp=common_verbalisations,\n",
    "    model_fp=ontr_snomed_minified_128_model_fp,\n",
    "    score_fn=concept_subsumption\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1c8119",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "### Configuration & Data\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4c5941cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load query objects via QueryObjectMapping:\n",
    "\n",
    "data_query_mapping = QueryObjectMapping(Path(\"../data/eval_dataset_50.json\"))\n",
    "\n",
    "equiv_queries, subsumpt_queries = data_query_mapping.get_queries()\n",
    "\n",
    "global_cutoff_depth = 5 # global cutoff parameter for multi-target retrieval\n",
    "\n",
    "# copy the original data (as we re-use later)\n",
    "\n",
    "oov_single_target_queries = copy.deepcopy(subsumpt_queries)\n",
    "for q in oov_single_target_queries:\n",
    "    q._ancestors = []\n",
    "    q._parents = []\n",
    "\n",
    "oov_match_all = copy.deepcopy(subsumpt_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6982dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the 'models dict' ready for experimental runs (single target):\n",
    "\n",
    "models_dict_single_target = {\n",
    "  # BASELINES\n",
    "  \"BoW TFIDF\": tfidf_ret,\n",
    "  \"BoW BM25\": bm25_ret,\n",
    "  # BASELINE CONTEXTUAL EMBEDDINGS\n",
    "  \"SBERT cos-sim\": sbert_ret_plm_w_cosine_sim,\n",
    "  # HiT (Full)\n",
    "  \"HiT SNO-25(F)\": hit_ret_snomed_25_w_hyp_dist,\n",
    "  # OnT Transfer Models\n",
    "  \"OnT GALEN(P)\": ont_ret_galen_pred_w_hyp_dist,\n",
    "  \"OnT ANATOMY(P)\": ont_ret_anatomy_pred_w_hyp_dist,\n",
    "  \"OnT GO(P)\": ont_ret_go_pred_w_hyp_dist,\n",
    "  # OnT SNOMED Models (Full, batch_size=64, Mini, batch_size=[32,64,128])\n",
    "  \"OnT SNO-25(FULL-LG)\": ont_snomed_LG_w_hyp_dist,\n",
    "  \"OnT SNO-25(M-32)\": ontr_ret_snomed_minified_32_w_hyp_dist,\n",
    "  \"OnT SNO-25(M-64)\": ontr_ret_snomed_minified_w_hyp_dist,\n",
    "  \"OnT SNO-25(M-128)\": ontr_ret_snomed_minified_128_w_hyp_dist,\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f680f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: BoW TFIDF\n",
      "  MRR:    0.26\n",
      "  H@1:    0.16\n",
      "  H@3:    0.30\n",
      "  H@5:    0.34\n",
      "  Med:    18.5\n",
      "  MR:     90308.1\n",
      "  R@100:  0.62\n",
      "------------------------------------------------------------\n",
      "Model: BoW BM25\n",
      "  MRR:    0.30\n",
      "  H@1:    0.20\n",
      "  H@3:    0.36\n",
      "  H@5:    0.44\n",
      "  Med:    11.5\n",
      "  MR:     44793.7\n",
      "  R@100:  0.6\n",
      "------------------------------------------------------------\n",
      "Model: SBERT cos-sim\n",
      "  MRR:    0.51\n",
      "  H@1:    0.34\n",
      "  H@3:    0.66\n",
      "  H@5:    0.70\n",
      "  Med:    2.0\n",
      "  MR:     27.3\n",
      "  R@100:  0.92\n",
      "------------------------------------------------------------\n",
      "Model: HiT SNO-25(F)\n",
      "  MRR:    0.39\n",
      "  H@1:    0.28\n",
      "  H@3:    0.46\n",
      "  H@5:    0.50\n",
      "  Med:    5.5\n",
      "  MR:     170.7\n",
      "  R@100:  0.78\n",
      "------------------------------------------------------------\n",
      "Model: OnTr GALEN(P)\n",
      "  MRR:    0.54\n",
      "  H@1:    0.34\n",
      "  H@3:    0.70\n",
      "  H@5:    0.80\n",
      "  Med:    2.0\n",
      "  MR:     11.2\n",
      "  R@100:  0.96\n",
      "------------------------------------------------------------\n",
      "Model: OnTr ANATOMY(P)\n",
      "  MRR:    0.62\n",
      "  H@1:    0.48\n",
      "  H@3:    0.70\n",
      "  H@5:    0.80\n",
      "  Med:    2.0\n",
      "  MR:     12.7\n",
      "  R@100:  0.98\n",
      "------------------------------------------------------------\n",
      "Model: OnTr GO(P)\n",
      "  MRR:    0.47\n",
      "  H@1:    0.32\n",
      "  H@3:    0.52\n",
      "  H@5:    0.62\n",
      "  Med:    3.0\n",
      "  MR:     22.4\n",
      "  R@100:  0.96\n",
      "------------------------------------------------------------\n",
      "Model: OnTr SNO-25(FULL-LG)\n",
      "  MRR:    0.54\n",
      "  H@1:    0.42\n",
      "  H@3:    0.58\n",
      "  H@5:    0.70\n",
      "  Med:    2.0\n",
      "  MR:     13.3\n",
      "  R@100:  0.96\n",
      "------------------------------------------------------------\n",
      "Model: OnTr SNO-25(M-32)\n",
      "  MRR:    0.60\n",
      "  H@1:    0.46\n",
      "  H@3:    0.66\n",
      "  H@5:    0.82\n",
      "  Med:    2.0\n",
      "  MR:     10.2\n",
      "  R@100:  0.96\n",
      "------------------------------------------------------------\n",
      "Model: OnTr SNO-25(M-64)\n",
      "  MRR:    0.61\n",
      "  H@1:    0.48\n",
      "  H@3:    0.68\n",
      "  H@5:    0.80\n",
      "  Med:    2.0\n",
      "  MR:     12.3\n",
      "  R@100:  0.96\n",
      "------------------------------------------------------------\n",
      "Model: OnTr SNO-25(M-128)\n",
      "  MRR:    0.63\n",
      "  H@1:    0.52\n",
      "  H@3:    0.72\n",
      "  H@5:    0.78\n",
      "  Med:    1.0\n",
      "  MR:     18.4\n",
      "  R@100:  0.96\n",
      "------------------------------------------------------------\n",
      "All results saved to ../data/oov_entity_mentions_single_target_ANN_50_queries.json\n",
      "Printing table: \n",
      "\n",
      "\n",
      "Model       Variant       MRR    H@1    H@3    H@5     Med       MR      R@100\n",
      "==============================================================================\n",
      "BoW     TFIDF             0.26   0.16   0.30   0.34   18.50   90308.10   0.62 \n",
      "BoW     BM25              0.30   0.20   0.36   0.44   11.50   44793.74   0.60 \n",
      "SBERT   cos-sim           0.51   0.34   0.66   0.70   2.00     27.30     0.92 \n",
      "HiT     SNO-25(F)         0.39   0.28   0.46   0.50   5.50     170.70    0.78 \n",
      "OnTr    GALEN(P)          0.54   0.34   0.70   0.80   2.00     11.22     0.96 \n",
      "OnTr    ANATOMY(P)        0.62   0.48   0.70   0.80   2.00     12.68     0.98 \n",
      "OnTr    GO(P)             0.47   0.32   0.52   0.62   3.00     22.40     0.96 \n",
      "OnTr    SNO-25(FULL-LG)   0.54   0.42   0.58   0.70   2.00     13.28     0.96 \n",
      "OnTr    SNO-25(M-32)      0.60   0.46   0.66   0.82   2.00     10.16     0.96 \n",
      "OnTr    SNO-25(M-64)      0.61   0.48   0.68   0.80   2.00     12.30     0.96 \n",
      "OnTr    SNO-25(M-128)     0.63   0.52   0.72   0.78   1.00     18.44     0.96 \n",
      "\n",
      "\n",
      " Printing LaTeX: \n",
      "\n",
      "\n",
      "\\begin{table}[H]\n",
      "\t\\caption[Single target performance of OOV mentions]{Single target retrieval performance of OOV entity mentions measured across multiple models (50 Queries)}\n",
      "\t\\begin{center}\n",
      "\t\t\\begin{tabular}{llccccccc}\n",
      "\t\t\t\\toprule\n",
      "\t\t\tModel & Variant & MRR & H@1 & H@3 & H@5 & Med & MR & R@100 \\\\\n",
      "\t\t\t\\midrule\n",
      "\t\t\tBoW & TFIDF & 0.26 & 0.16 & 0.30 & 0.34 & 18.50 & 90308.10 & 0.62 \\\\\n",
      "\t\t\tBoW & BM25 & 0.30 & 0.20 & 0.36 & 0.44 & 11.50 & 44793.74 & 0.60 \\\\\n",
      "\t\t\tSBERT & cos-sim & 0.51 & 0.34 & 0.66 & 0.70 & 2.00 & 27.30 & 0.92 \\\\\n",
      "\t\t\tHiT & SNO-25(F) & 0.39 & 0.28 & 0.46 & 0.50 & 5.50 & 170.70 & 0.78 \\\\\n",
      "\t\t\tOnTr & GALEN(P) & 0.54 & 0.34 & 0.70 & 0.80 & 2.00 & 11.22 & 0.96 \\\\\n",
      "\t\t\tOnTr & ANATOMY(P) & 0.62 & 0.48 & 0.70 & 0.80 & 2.00 & 12.68 & 0.98 \\\\\n",
      "\t\t\tOnTr & GO(P) & 0.47 & 0.32 & 0.52 & 0.62 & 3.00 & 22.40 & 0.96 \\\\\n",
      "\t\t\tOnTr & SNO-25(FULL-LG) & 0.54 & 0.42 & 0.58 & 0.70 & 2.00 & 13.28 & 0.96 \\\\\n",
      "\t\t\tOnTr & SNO-25(M-32) & 0.60 & 0.46 & 0.66 & 0.82 & 2.00 & 10.16 & 0.96 \\\\\n",
      "\t\t\tOnTr & SNO-25(M-64) & 0.61 & 0.48 & 0.68 & 0.80 & 2.00 & 12.30 & 0.96 \\\\\n",
      "\t\t\tOnTr & SNO-25(M-128) & 0.63 & 0.52 & 0.72 & 0.78 & 1.00 & 18.44 & 0.96 \\\\\n",
      "\t\t\t\\bottomrule\n",
      "\t\t\\end{tabular}\n",
      "\t\\end{center}\n",
      "\t\\label{tab:single-target-oov}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "# OOV QUERIES (SINGLE TARGET) [50]\n",
    "\n",
    "# PREP TABLE START #\n",
    "import latextable\n",
    "from latextable import texttable\n",
    "experiment_one_table = texttable.Texttable()\n",
    "experiment_one_table.set_deco(texttable.Texttable.HEADER)\n",
    "experiment_one_table.set_precision(2)\n",
    "experiment_one_table.set_cols_dtype(['t', 't', 'f', 'f', 'f', 'f', 'f', 'f', 'f'])\n",
    "experiment_one_table.set_cols_align([\"l\", \"l\", \"c\", \"c\", \"c\", \"c\", \"c\", \"c\", \"c\"])\n",
    "experiment_one_table.header([\"Model\", \"Variant\", \"MRR\", \"H@1\", \"H@3\", \"H@5\", \"Med\", \"MR\", \"R@100\"])\n",
    "# END-PREP TABLE #\n",
    "\n",
    "ks      = [1, 3, 5, 100, len(entity_verbalisation_list)]\n",
    "MAX_K   = max(ks)\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for model_name, model in models_dict_single_target.items():\n",
    "    \n",
    "    # init accumulators\n",
    "    results = {\n",
    "      \"MRR\": 0.0, # Mean Reciprical Rank\n",
    "      **{f\"H@{k}\": 0.0 for k in ks}, # Hits@k\n",
    "      **{f\"P@{k}\": 0.0 for k in ks}, # Precision@k\n",
    "      **{f\"R@{k}\": 0.0 for k in ks}, # Recall@k\n",
    "      **{f\"F1@{k}\": 0.0 for k in ks}, # F1@k\n",
    "      \"MR\": 0.0 # Mean Rank\n",
    "    }\n",
    "    # PR-AUC, Median Rank & Coverage are calculated during the test procedure\n",
    "    hit_count = 0 # for coverage\n",
    "    all_ranks = [] # for median rank\n",
    "\n",
    "    for q_idx, query in enumerate(oov_single_target_queries):\n",
    "        \n",
    "        qstr = query.get_query_string()\n",
    "        gold_iri = query.get_target_iri()\n",
    "\n",
    "        ranked_results = [] # empty lists (are unlikely to exist) but are treated as full misses\n",
    "        \n",
    "        # TODO: replace with match (?) - i.e. switch\n",
    "        if isinstance(model, HiTRetriever):\n",
    "          if model._score_fn == entity_subsumption:\n",
    "            ranked_results = model.retrieve(qstr, top_k=None, reverse_candidate_scores=True, model=model._model, weight=0.0)\n",
    "          elif model._score_fn == batch_poincare_dist_with_adaptive_curv_k: \n",
    "            ranked_results = model.retrieve(qstr, top_k=None, reverse_candidate_scores=False, model=model._model)\n",
    "        #\n",
    "        elif isinstance(model, OnTRetriever):\n",
    "          if model._score_fn == concept_subsumption:\n",
    "            ranked_results = model.retrieve(qstr, top_k=None, reverse_candidate_scores=True, model=model._model, weight=0.0)\n",
    "          elif model._score_fn == batch_poincare_dist_with_adaptive_curv_k: \n",
    "            ranked_results = model.retrieve(qstr, top_k=None, reverse_candidate_scores=False, model=model._model)\n",
    "        #\n",
    "        elif isinstance(model, SBERTRetriever):\n",
    "          if model._score_fn == batch_cosine_similarity:\n",
    "            ranked_results = model.retrieve(qstr, top_k=None, reverse_candidate_scores=True)\n",
    "          else:\n",
    "            ranked_results = model.retrieve(qstr, top_k=None, reverse_candidate_scores=False)\n",
    "        #\n",
    "        elif isinstance(model, BM25Retriever):\n",
    "          ranked_results = model.retrieve(qstr, top_k=None)\n",
    "        #\n",
    "        elif isinstance(model, TFIDFRetriever):\n",
    "          ranked_results = model.retrieve(qstr, top_k=None)\n",
    "        #\n",
    "        elif isinstance(model, MixedModelRetriever):\n",
    "           ranked_results = model.retrieve(qstr, top_k=None)\n",
    "        #\n",
    "        elif isinstance(model, CustomMixedModelRetriever):\n",
    "           ranked_results = model.retrieve(qstr, top_k=MAX_K)\n",
    "        #\n",
    "        else:\n",
    "           raise ValueError(\"No appropriate retriever has been set.\")\n",
    "\n",
    "        retrieved_iris = [iri for (_, iri, _, _) in ranked_results] # type: ignore\n",
    "\n",
    "        # MRR & MeanRank\n",
    "        rank_pos = None\n",
    "        for rank_idx, iri in enumerate(retrieved_iris, start=1):\n",
    "            if iri == gold_iri:\n",
    "                rank_pos = rank_idx\n",
    "                results[\"MRR\"] += 1.0 / rank_idx\n",
    "                results[\"MR\"] += rank_idx\n",
    "                break\n",
    "        \n",
    "        # for calculating coverage\n",
    "        if rank_pos is not None:\n",
    "           hit_count += 1\n",
    "\n",
    "        # include a penalty to appropriately offset the MR\n",
    "        # rather than artifically inflating the performance\n",
    "        # by simply dropping queries that do not contain \n",
    "        if rank_pos is None:\n",
    "            results[\"MR\"] += MAX_K + 1 # penalty: rank := MAX_K + 1\n",
    "\n",
    "        for k in ks:\n",
    "            hit = 1 if (rank_pos is not None and rank_pos <= k) else 0\n",
    "            results[f\"H@{k}\"] += hit # Hits@K\n",
    "            p_at_k = hit / k # Precision@K\n",
    "            results[f\"P@{k}\"] += p_at_k\n",
    "            r_at_k = 1 if (rank_pos is not None and rank_pos <= k) else 0\n",
    "            results[f\"R@{k}\"] += r_at_k\n",
    "            if (p_at_k + r_at_k) > 0:\n",
    "               results[f\"F1@{k}\"] += 2 * (p_at_k * r_at_k) / (p_at_k + r_at_k)\n",
    "\n",
    "        final_rank = rank_pos if rank_pos is not None else MAX_K + 1\n",
    "        all_ranks.append(final_rank)\n",
    "\n",
    "    # normalise over queries & compute coverage\n",
    "    N = len(oov_single_target_queries)\n",
    "    normalized = {metric: value / N for metric, value in results.items()}\n",
    "    normalized['Cov'] = (hit_count / N) # calculate the coverage of this model\n",
    "    normalized['Med'] = statistics.median(all_ranks) # median rank\n",
    "    # area under precision-recall curve (trapezodial rule)\n",
    "    recall_at_k_xs    = [normalized[f\"R@{k}\"] for k in ks]\n",
    "    # check for monotonic recall\n",
    "    if any(r2 < r1 for r1, r2 in zip(recall_at_k_xs, recall_at_k_xs[1:])):\n",
    "        raise ValueError(f\"Recall must be non-decreasing for PR-AUC\")\n",
    "    precision_at_k_xs = [normalized[f\"P@{k}\"] for k in ks]\n",
    "    normalized[\"AUC\"] = float(sk_auc(recall_at_k_xs, precision_at_k_xs))\n",
    "\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"  MRR:    {normalized['MRR']:.2f}\")\n",
    "    for k in [1, 3, 5]:\n",
    "        print(f\"  H@{k}:    {normalized[f'H@{k}']:.2f}\")\n",
    "    print(f\"  Med:    {normalized['Med']:.1f}\")\n",
    "    print(f\"  MR:     {normalized['MR']:.1f}\")\n",
    "    print(f\"  R@100:  {normalized['R@100']}\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    model_metric_string = model_name.split()\n",
    "    experiment_one_table.add_row([model_metric_string[0], model_metric_string[1], \n",
    "                                  normalized['MRR'], \n",
    "                                  normalized['H@1'], normalized['H@3'], normalized['H@5'], \n",
    "                                  normalized['Med'], normalized['MR'], normalized['R@100']])\n",
    "\n",
    "    all_results[model_name] = normalized\n",
    "\n",
    "Path('../logs').mkdir(parents=True, exist_ok=True)\n",
    "output_file = '../logs/oov_entity_mentions_single_target_ANN_50_queries.json'\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "\n",
    "print(f\"All results saved to {output_file}\")\n",
    "\n",
    "print(f\"Printing table: \\n\\n\")\n",
    "\n",
    "print(experiment_one_table.draw())\n",
    "\n",
    "print(\"\\n\\n Printing LaTeX: \\n\\n\")\n",
    "\n",
    "print(latextable.draw_latex(\n",
    "    table=experiment_one_table, \n",
    "    caption=\"Single target retrieval performance of OOV entity mentions measured across multiple models (50 Queries)\", \n",
    "    use_booktabs=True, position=\"H\", caption_above=True, caption_short=\"Single target performance of OOV mentions\", \n",
    "    label=\"tab:single-target-oov\"\n",
    "  )\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c613d37",
   "metadata": {},
   "source": [
    "## Multi-target Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "71aeb660",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict_multi_target = {\n",
    "  # baselines\n",
    "  \"BoW Lexical TFIDF\": tfidf_ret,\n",
    "  \"BoW Lexical BM25\": bm25_ret,\n",
    "  # baseline contextual\n",
    "  \"SBERT SNOMED25 cos-sim\": sbert_ret_plm_w_cosine_sim,\n",
    "  \"SBERT SNOMED25 d_l2\": sbert_ret_plm_w_euclid_dist,\n",
    "  # HiT models\n",
    "  \"HiT SNOMED25(F) d_k\": hit_ret_snomed_25_w_hyp_dist,\n",
    "  \"HiT SNOMED25(F) s_e\": hit_ret_snomed_25_w_ent_sub,\n",
    "  # OnT transferability (prediction)\n",
    "  \"OnTr GALEN(P) d_k\": ont_ret_galen_pred_w_hyp_dist,\n",
    "  \"OnTr GALEN(P) s_c\": ont_ret_galen_pred_w_con_sub,\n",
    "  \"OnTr ANATOMY(P) d_k\": ont_ret_anatomy_pred_w_hyp_dist,\n",
    "  \"OnTr ANATOMY(P) s_c\": ont_ret_anatomy_pred_w_con_sub,\n",
    "  \"OnTr GO(P) d_k\": ont_ret_go_pred_w_hyp_dist,\n",
    "  \"OnTr GO(P) s_c\": ont_ret_go_pred_w_con_sub,\n",
    "  # OnT SNOMED models\n",
    "  \"OnTr SNO-25(FULL-LG) d_k\": ont_snomed_LG_w_hyp_dist,\n",
    "  \"OnTr SNO-25(FULL-LG) s_c\": ont_snomed_LG_w_con_sub,\n",
    "  \"OnTr SNO-25(M-32) d_k\": ontr_ret_snomed_minified_32_w_hyp_dist,\n",
    "  \"OnTr SNO-25(M-32) s_c\": ontr_ret_snomed_minified_32_w_con_sub,\n",
    "  \"OnTr SNO-25(M-64) d_k\": ontr_ret_snomed_minified_w_hyp_dist,\n",
    "  \"OnTr SNO-25(M-64) s_c\": ontr_ret_snomed_minified_w_con_sub,\n",
    "  \"OnTr SNO-25(M-128) d_k\": ontr_ret_snomed_minified_128_w_hyp_dist,\n",
    "  \"OnTr SNO-25(M-128) s_c\": ontr_ret_snomed_minified_128_w_con_sub,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c8ea1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision_binary(rels: Iterable[int], total_relevant: int | None = None) -> float:\n",
    "    rels = [1 if r > 0 else 0 for r in rels]\n",
    "    if total_relevant is None:\n",
    "        # AP over the full ranking (or AP@K using seen rels as denominator)\n",
    "        total_relevant = sum(rels)\n",
    "    if total_relevant == 0:\n",
    "        return 0.0\n",
    "    hits = 0\n",
    "    cum_prec = 0.0\n",
    "    for k, r in enumerate(rels, start=1):\n",
    "        if r:\n",
    "            hits += 1\n",
    "            cum_prec += hits / k\n",
    "    return cum_prec / total_relevant\n",
    "\n",
    "\n",
    "def pr_points_from_binary(rels: Iterable[int], total_relevant: int | None = None) -> tuple[np.ndarray, np.ndarray]:\n",
    "    rels = np.asarray([1 if r > 0 else 0 for r in rels], dtype=int)\n",
    "    if total_relevant is None:\n",
    "        total_relevant = int(rels.sum())  # fallback\n",
    "    if total_relevant == 0:\n",
    "        return np.array([]), np.array([])\n",
    "    hits_cum = np.cumsum(rels)\n",
    "    hit_mask = rels == 1\n",
    "    ranks = np.nonzero(hit_mask)[0] + 1   # 1-based ranks of hits\n",
    "    precisions = hits_cum[hit_mask] / ranks.astype(float)\n",
    "    recalls    = hits_cum[hit_mask] / float(total_relevant)\n",
    "    return recalls, precisions\n",
    "\n",
    "\n",
    "def interpolate_precision(recall: np.ndarray, precision: np.ndarray, recall_grid: np.ndarray) -> np.ndarray:\n",
    "  if recall.size == 0:\n",
    "    return np.zeros_like(recall_grid, dtype=float)\n",
    "  # sort recall, ensuring monotonicity\n",
    "  order = np.argsort(recall)\n",
    "  r = recall[order] # recall in asc (smallest -> largest)\n",
    "  p = precision[order] # precision @ i : \\forall i \\in r(ecall)\n",
    "  # non-increasing precision (cumulative maxima) from right to left\n",
    "  p_right_max = p.copy()\n",
    "  # i <- arg (position) \\in prec, reversed in desc\n",
    "  for i in range(len(p_right_max) - 2, -1, -1):\n",
    "    if p_right_max[i] < p_right_max[i + 1]:\n",
    "        p_right_max[i] = p_right_max[i + 1]\n",
    "  # ^ yields (recall, max(precision)) @ k : \\forall k \\in \\{r_0, r_1, \\cdot r_n\\} \\leftarrow \\text{recall}\n",
    "  # i.e. p, r : p -> max(p), r -> r \\forall r \\in R (r*)\n",
    "  interp = np.zeros_like(recall_grid, dtype=float)\n",
    "  for i, rg in enumerate(recall_grid):\n",
    "      # find first index where recall >= rg\n",
    "      idx = np.searchsorted(r, rg, side=\"left\")\n",
    "      if idx < len(p_right_max):\n",
    "          interp[i] = p_right_max[idx]\n",
    "      else:\n",
    "          interp[i] = 0.0\n",
    "  return interp\n",
    "\n",
    "\n",
    "def macro_pr_curve(all_query_rels: list[tuple[Iterable[int], int]], recall_points: int = 101) -> tuple[np.ndarray, np.ndarray]:\n",
    "    recall_grid = np.linspace(0.0, 1.0, recall_points)\n",
    "    acc = np.zeros_like(recall_grid, dtype=float)\n",
    "    Q = len(all_query_rels)\n",
    "    if Q == 0:\n",
    "        return recall_grid, acc\n",
    "    for rels, total_relevant in all_query_rels:\n",
    "        r, p = pr_points_from_binary(rels, total_relevant=total_relevant)\n",
    "        acc += interpolate_precision(r, p, recall_grid)\n",
    "    return recall_grid, acc / Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2ad5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: BoW Lexical TFIDF\n",
      " mAP: \t  0.08\n",
      " MRR*:   0.29\n",
      " nDCG@10: 0.22\n",
      " PR-AUC:  0.01\n",
      " mPR-AUC: 0.08\n",
      " R@100:   0.20\n",
      "------------------------------------------------------------\n",
      "Model: BoW Lexical BM25\n",
      " mAP: \t  0.08\n",
      " MRR*:   0.32\n",
      " nDCG@10: 0.24\n",
      " PR-AUC:  0.02\n",
      " mPR-AUC: 0.08\n",
      " R@100:   0.19\n",
      "------------------------------------------------------------\n",
      "Model: SBERT SNOMED25 cos-sim\n",
      " mAP: \t  0.15\n",
      " MRR*:   0.55\n",
      " nDCG@10: 0.41\n",
      " PR-AUC:  0.05\n",
      " mPR-AUC: 0.15\n",
      " R@100:   0.37\n",
      "------------------------------------------------------------\n",
      "Model: SBERT SNOMED25 d_l2\n",
      " mAP: \t  0.15\n",
      " MRR*:   0.55\n",
      " nDCG@10: 0.41\n",
      " PR-AUC:  0.05\n",
      " mPR-AUC: 0.15\n",
      " R@100:   0.37\n",
      "------------------------------------------------------------\n",
      "Model: HiT SNOMED25(F) d_k\n",
      " mAP: \t  0.14\n",
      " MRR*:   0.44\n",
      " nDCG@10: 0.32\n",
      " PR-AUC:  0.05\n",
      " mPR-AUC: 0.15\n",
      " R@100:   0.45\n",
      "------------------------------------------------------------\n",
      "Model: HiT SNOMED25(F) s_e\n",
      " mAP: \t  0.15\n",
      " MRR*:   0.39\n",
      " nDCG@10: 0.28\n",
      " PR-AUC:  0.04\n",
      " mPR-AUC: 0.15\n",
      " R@100:   0.38\n",
      "------------------------------------------------------------\n",
      "Model: OnTr GALEN(P) d_k\n",
      " mAP: \t  0.17\n",
      " MRR*:   0.61\n",
      " nDCG@10: 0.44\n",
      " PR-AUC:  0.07\n",
      " mPR-AUC: 0.18\n",
      " R@100:   0.43\n",
      "------------------------------------------------------------\n",
      "Model: OnTr GALEN(P) s_c\n",
      " mAP: \t  0.17\n",
      " MRR*:   0.58\n",
      " nDCG@10: 0.43\n",
      " PR-AUC:  0.07\n",
      " mPR-AUC: 0.18\n",
      " R@100:   0.45\n",
      "------------------------------------------------------------\n",
      "Model: OnTr ANATOMY(P) d_k\n",
      " mAP: \t  0.19\n",
      " MRR*:   0.68\n",
      " nDCG@10: 0.49\n",
      " PR-AUC:  0.07\n",
      " mPR-AUC: 0.20\n",
      " R@100:   0.43\n",
      "------------------------------------------------------------\n",
      "Model: OnTr ANATOMY(P) s_c\n",
      " mAP: \t  0.19\n",
      " MRR*:   0.68\n",
      " nDCG@10: 0.47\n",
      " PR-AUC:  0.06\n",
      " mPR-AUC: 0.20\n",
      " R@100:   0.44\n",
      "------------------------------------------------------------\n",
      "Model: OnTr GO(P) d_k\n",
      " mAP: \t  0.16\n",
      " MRR*:   0.57\n",
      " nDCG@10: 0.39\n",
      " PR-AUC:  0.05\n",
      " mPR-AUC: 0.16\n",
      " R@100:   0.42\n",
      "------------------------------------------------------------\n",
      "Model: OnTr GO(P) s_c\n",
      " mAP: \t  0.16\n",
      " MRR*:   0.55\n",
      " nDCG@10: 0.38\n",
      " PR-AUC:  0.05\n",
      " mPR-AUC: 0.16\n",
      " R@100:   0.42\n",
      "------------------------------------------------------------\n",
      "Model: OnTr SNO-25(FULL-LG) d_k\n",
      " mAP: \t  0.18\n",
      " MRR*:   0.58\n",
      " nDCG@10: 0.44\n",
      " PR-AUC:  0.07\n",
      " mPR-AUC: 0.18\n",
      " R@100:   0.49\n",
      "------------------------------------------------------------\n",
      "Model: OnTr SNO-25(FULL-LG) s_c\n",
      " mAP: \t  0.19\n",
      " MRR*:   0.57\n",
      " nDCG@10: 0.42\n",
      " PR-AUC:  0.07\n",
      " mPR-AUC: 0.20\n",
      " R@100:   0.52\n",
      "------------------------------------------------------------\n",
      "Model: OnTr SNO-25(M-32) d_k\n",
      " mAP: \t  0.19\n",
      " MRR*:   0.66\n",
      " nDCG@10: 0.47\n",
      " PR-AUC:  0.07\n",
      " mPR-AUC: 0.19\n",
      " R@100:   0.46\n",
      "------------------------------------------------------------\n",
      "Model: OnTr SNO-25(M-32) s_c\n",
      " mAP: \t  0.19\n",
      " MRR*:   0.63\n",
      " nDCG@10: 0.42\n",
      " PR-AUC:  0.06\n",
      " mPR-AUC: 0.20\n",
      " R@100:   0.44\n",
      "------------------------------------------------------------\n",
      "Model: OnTr SNO-25(M-64) d_k\n",
      " mAP: \t  0.19\n",
      " MRR*:   0.68\n",
      " nDCG@10: 0.47\n",
      " PR-AUC:  0.07\n",
      " mPR-AUC: 0.20\n",
      " R@100:   0.46\n",
      "------------------------------------------------------------\n",
      "Model: OnTr SNO-25(M-64) s_c\n",
      " mAP: \t  0.19\n",
      " MRR*:   0.60\n",
      " nDCG@10: 0.42\n",
      " PR-AUC:  0.06\n",
      " mPR-AUC: 0.19\n",
      " R@100:   0.44\n",
      "------------------------------------------------------------\n",
      "Model: OnTr SNO-25(M-128) d_k\n",
      " mAP: \t  0.19\n",
      " MRR*:   0.68\n",
      " nDCG@10: 0.48\n",
      " PR-AUC:  0.07\n",
      " mPR-AUC: 0.20\n",
      " R@100:   0.44\n",
      "------------------------------------------------------------\n",
      "Model: OnTr SNO-25(M-128) s_c\n",
      " mAP: \t  0.19\n",
      " MRR*:   0.61\n",
      " nDCG@10: 0.44\n",
      " PR-AUC:  0.07\n",
      " mPR-AUC: 0.20\n",
      " R@100:   0.45\n",
      "------------------------------------------------------------\n",
      "All results saved to ../data/oov_entity_mentions_multi_relevant_targets_weight_w035_50_queries.json\n",
      "Macro PR AUC plot data dumped to ../data/oov_entity_mentions_multi_target_WEIGHTED_w035_50_q__PR_AUC_POINTS_4_PLOT.json\n",
      "Printing table: \n",
      "\n",
      "\n",
      "Model       Variant       Metric    mAP    MRR*   nDCG@10   PR-AUC   R@100\n",
      "==========================================================================\n",
      "BoW     Lexical           TFIDF     0.08   0.29    0.22      0.08    0.20 \n",
      "BoW     Lexical           BM25      0.08   0.32    0.24      0.08    0.19 \n",
      "SBERT   SNOMED25          cos-sim   0.15   0.55    0.41      0.15    0.37 \n",
      "SBERT   SNOMED25          d_l2      0.15   0.55    0.41      0.15    0.37 \n",
      "HiT     SNOMED25(F)       d_k       0.14   0.44    0.32      0.15    0.45 \n",
      "HiT     SNOMED25(F)       s_e       0.15   0.39    0.28      0.15    0.38 \n",
      "OnTr    GALEN(P)          d_k       0.17   0.61    0.44      0.18    0.43 \n",
      "OnTr    GALEN(P)          s_c       0.17   0.58    0.43      0.18    0.45 \n",
      "OnTr    ANATOMY(P)        d_k       0.19   0.68    0.49      0.20    0.43 \n",
      "OnTr    ANATOMY(P)        s_c       0.19   0.68    0.47      0.20    0.44 \n",
      "OnTr    GO(P)             d_k       0.16   0.57    0.39      0.16    0.42 \n",
      "OnTr    GO(P)             s_c       0.16   0.55    0.38      0.16    0.42 \n",
      "OnTr    SNO-25(FULL-LG)   d_k       0.18   0.58    0.44      0.18    0.49 \n",
      "OnTr    SNO-25(FULL-LG)   s_c       0.19   0.57    0.42      0.20    0.52 \n",
      "OnTr    SNO-25(M-32)      d_k       0.19   0.66    0.47      0.19    0.46 \n",
      "OnTr    SNO-25(M-32)      s_c       0.19   0.63    0.42      0.20    0.44 \n",
      "OnTr    SNO-25(M-64)      d_k       0.19   0.68    0.47      0.20    0.46 \n",
      "OnTr    SNO-25(M-64)      s_c       0.19   0.60    0.42      0.19    0.44 \n",
      "OnTr    SNO-25(M-128)     d_k       0.19   0.68    0.48      0.20    0.44 \n",
      "OnTr    SNO-25(M-128)     s_c       0.19   0.61    0.44      0.20    0.45 \n",
      "\n",
      "\n",
      " Printing LaTeX: \n",
      "\n",
      "\n",
      "\\begin{table}[H]\n",
      "\t\\caption[Multi target performance of OOV mentions, lambda=0.35]{Performance of fetching multiple relevant entities using OOV mentions with lambda=0.35 (50 Queries)}\n",
      "\t\\begin{center}\n",
      "\t\t\\begin{tabular}{lllccccc}\n",
      "\t\t\t\\toprule\n",
      "\t\t\tModel & Variant & Metric & mAP & MRR* & nDCG@10 & PR-AUC & R@100 \\\\\n",
      "\t\t\t\\midrule\n",
      "\t\t\tBoW & Lexical & TFIDF & 0.08 & 0.29 & 0.22 & 0.08 & 0.20 \\\\\n",
      "\t\t\tBoW & Lexical & BM25 & 0.08 & 0.32 & 0.24 & 0.08 & 0.19 \\\\\n",
      "\t\t\tSBERT & SNOMED25 & cos-sim & 0.15 & 0.55 & 0.41 & 0.15 & 0.37 \\\\\n",
      "\t\t\tSBERT & SNOMED25 & d_l2 & 0.15 & 0.55 & 0.41 & 0.15 & 0.37 \\\\\n",
      "\t\t\tHiT & SNOMED25(F) & d_k & 0.14 & 0.44 & 0.32 & 0.15 & 0.45 \\\\\n",
      "\t\t\tHiT & SNOMED25(F) & s_e & 0.15 & 0.39 & 0.28 & 0.15 & 0.38 \\\\\n",
      "\t\t\tOnTr & GALEN(P) & d_k & 0.17 & 0.61 & 0.44 & 0.18 & 0.43 \\\\\n",
      "\t\t\tOnTr & GALEN(P) & s_c & 0.17 & 0.58 & 0.43 & 0.18 & 0.45 \\\\\n",
      "\t\t\tOnTr & ANATOMY(P) & d_k & 0.19 & 0.68 & 0.49 & 0.20 & 0.43 \\\\\n",
      "\t\t\tOnTr & ANATOMY(P) & s_c & 0.19 & 0.68 & 0.47 & 0.20 & 0.44 \\\\\n",
      "\t\t\tOnTr & GO(P) & d_k & 0.16 & 0.57 & 0.39 & 0.16 & 0.42 \\\\\n",
      "\t\t\tOnTr & GO(P) & s_c & 0.16 & 0.55 & 0.38 & 0.16 & 0.42 \\\\\n",
      "\t\t\tOnTr & SNO-25(FULL-LG) & d_k & 0.18 & 0.58 & 0.44 & 0.18 & 0.49 \\\\\n",
      "\t\t\tOnTr & SNO-25(FULL-LG) & s_c & 0.19 & 0.57 & 0.42 & 0.20 & 0.52 \\\\\n",
      "\t\t\tOnTr & SNO-25(M-32) & d_k & 0.19 & 0.66 & 0.47 & 0.19 & 0.46 \\\\\n",
      "\t\t\tOnTr & SNO-25(M-32) & s_c & 0.19 & 0.63 & 0.42 & 0.20 & 0.44 \\\\\n",
      "\t\t\tOnTr & SNO-25(M-64) & d_k & 0.19 & 0.68 & 0.47 & 0.20 & 0.46 \\\\\n",
      "\t\t\tOnTr & SNO-25(M-64) & s_c & 0.19 & 0.60 & 0.42 & 0.19 & 0.44 \\\\\n",
      "\t\t\tOnTr & SNO-25(M-128) & d_k & 0.19 & 0.68 & 0.48 & 0.20 & 0.44 \\\\\n",
      "\t\t\tOnTr & SNO-25(M-128) & s_c & 0.19 & 0.61 & 0.44 & 0.20 & 0.45 \\\\\n",
      "\t\t\t\\bottomrule\n",
      "\t\t\\end{tabular}\n",
      "\t\\end{center}\n",
      "\t\\label{tab:multi-target-oov-weighted}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "# OOV (TARGET + ANCESTORS) [weighted subsumption retrieval, d=5, \\lambda = 0.35] [50]\n",
    "\n",
    "# PREP TABLE START #\n",
    "experiment_three_table = texttable.Texttable()\n",
    "experiment_three_table.set_deco(texttable.Texttable.HEADER)\n",
    "experiment_three_table.set_precision(2)\n",
    "experiment_three_table.set_cols_dtype(['t', 't', 't', 'f', 'f', 'f', 'f', 'f'])\n",
    "experiment_three_table.set_cols_align([\"l\", \"l\", \"l\", \"c\", \"c\", \"c\", \"c\", \"c\"])\n",
    "experiment_three_table.header([\"Model\", \"Variant\", \"Metric\", \"mAP\", \"MRR*\", \"nDCG@10\", \"PR-AUC\", \"R@100\"])\n",
    "# END-PREP TABLE #\n",
    "\n",
    "ks      = [1, 3, 5, 10, 100, len(entity_verbalisation_list)]\n",
    "MAX_K   = max(ks)\n",
    "\n",
    "all_results = {}\n",
    "macro_avg_PR_AUC_data = {}\n",
    "\n",
    "for model_name, model in models_dict_multi_target.items():\n",
    "    \n",
    "    # init accumulators\n",
    "    results = {\n",
    "      \"MRR\": 0.0, # Mean Reciprical Rank\n",
    "      \"MAP\": 0.0, # Mean Average Precision\n",
    "      **{f\"Hits@{k}\": 0.0 for k in ks},\n",
    "      **{f\"P@{k}\": 0.0 for k in ks}, # Precision@k\n",
    "      **{f\"R@{k}\": 0.0 for k in ks}, # Recall@k\n",
    "      **{f\"F1@{k}\": 0.0 for k in ks}, # F1@k\n",
    "      **{f\"nDCG@{k}\": 0.0 for k in ks}, # normalised Discounted Cumlative Gain @ k\n",
    "      \"MR\": 0.0, # Mean Rank\n",
    "      \"aRP\": 0.0  # R-Precision\n",
    "    }\n",
    "    # AUC-PR, Median Rank & Coverage are calculated during the test procedure\n",
    "    hit_count = 0 # for coverage\n",
    "    total_possible_hits = 0 # for coverage := hit_count / total_possible_hits .. essentially: recall@k, when k = MAX_K\n",
    "    all_ranks = [] # for median rank\n",
    "    # @depricationWarning : AUC-PR, previous implementation was rough approximation\n",
    "    per_query_rels_for_PR = []\n",
    "\n",
    "    for q_idx, query in enumerate(oov_match_all):\n",
    "        \n",
    "        qstr = query.get_query_string()\n",
    "        gold_targets = query.get_unique_sorted_subsumptive_targets(key=\"depth\", reverse=False, depth_cutoff=global_cutoff_depth) # [*parents, *ancestors]\n",
    "        g_target_iris = set([x[\"iri\"] for x in gold_targets])\n",
    "        num_targets = len(g_target_iris)\n",
    "        total_possible_hits += num_targets\n",
    "        average_precision = 0.0\n",
    "        hit_count_this_query = 0\n",
    "        hit_count_lt_or_eq_num_targets = 0\n",
    "\n",
    "        ranked_results: list[QueryResult] = [] # empty lists (are unlikely to exist) but are treated as full misses\n",
    "        \n",
    "        # TODO: replace with match (?) - i.e. switch\n",
    "        if isinstance(model, HiTRetriever):\n",
    "          if model._score_fn == entity_subsumption:\n",
    "            ranked_results = model.retrieve(qstr, top_k=MAX_K, reverse_candidate_scores=True, model=model._model, weight=0.35)\n",
    "          elif model._score_fn == batch_poincare_dist_with_adaptive_curv_k: \n",
    "            ranked_results = model.retrieve(qstr, top_k=MAX_K, reverse_candidate_scores=False, model=model._model)\n",
    "        #\n",
    "        elif isinstance(model, OnTRetriever):\n",
    "          if model._score_fn == concept_subsumption:\n",
    "            ranked_results = model.retrieve(qstr, top_k=MAX_K, reverse_candidate_scores=True, model=model._model, weight=0.35)\n",
    "          elif model._score_fn == batch_poincare_dist_with_adaptive_curv_k: \n",
    "            ranked_results = model.retrieve(qstr, top_k=MAX_K, reverse_candidate_scores=False, model=model._model)\n",
    "        #\n",
    "        elif isinstance(model, SBERTRetriever):\n",
    "          if model._score_fn == batch_cosine_similarity:\n",
    "            ranked_results = model.retrieve(qstr, top_k=MAX_K, reverse_candidate_scores=True)\n",
    "          else:\n",
    "            ranked_results = model.retrieve(qstr, top_k=MAX_K, reverse_candidate_scores=False)\n",
    "        #\n",
    "        elif isinstance(model, BM25Retriever):\n",
    "          ranked_results = model.retrieve(qstr, top_k=MAX_K)\n",
    "        #\n",
    "        elif isinstance(model, TFIDFRetriever):\n",
    "          ranked_results = model.retrieve(qstr, top_k=MAX_K)\n",
    "        #\n",
    "        elif isinstance(model, MixedModelRetriever):\n",
    "          ranked_results = model.retrieve(qstr, top_k=MAX_K)\n",
    "        #\n",
    "        elif isinstance(model, CustomMixedModelRetriever):\n",
    "          ranked_results = model.retrieve(qstr, top_k=MAX_K)\n",
    "        #\n",
    "        else:\n",
    "           raise ValueError(\"No appropriate retriever has been set.\")\n",
    "\n",
    "        retrieved_iris = [iri for (_, iri, _, _) in ranked_results] # type: ignore\n",
    "\n",
    "        # (macro) PR-AUC\n",
    "        rel_binary = []\n",
    "        for rank_idx, iri in enumerate(retrieved_iris, start=1):\n",
    "            if iri in g_target_iris:\n",
    "              rel_binary.append(1)\n",
    "            else:\n",
    "              rel_binary.append(0)\n",
    "        per_query_rels_for_PR.append((rel_binary, num_targets))\n",
    "\n",
    "        # MRR & Mean Rank (on the first hit)\n",
    "        rank_pos = None\n",
    "        for rank_idx, iri in enumerate(retrieved_iris, start=1):\n",
    "            if iri in g_target_iris:\n",
    "                rank_pos = rank_idx\n",
    "                results[\"MRR\"] += 1.0 / rank_idx\n",
    "                results[\"MR\"] += rank_idx\n",
    "                break\n",
    "        \n",
    "        # Average Precision (this query), for use in calculating mAP\n",
    "        for rank_idx, iri in enumerate(retrieved_iris, start=1):\n",
    "           if iri in g_target_iris:\n",
    "              hit_count += 1\n",
    "              hit_count_this_query += 1\n",
    "              average_precision += hit_count_this_query / rank_idx\n",
    "        average_precision /= num_targets\n",
    "        results[\"MAP\"] += average_precision\n",
    "\n",
    "        # R-Precision (this query)\n",
    "        for rank_idx, iri in enumerate(retrieved_iris, start=1):\n",
    "           if iri in g_target_iris:\n",
    "              hit_count_lt_or_eq_num_targets += 1\n",
    "           if rank_idx == num_targets: # then we need to calculate the precision @ this index\n",
    "              results[\"aRP\"] += hit_count_lt_or_eq_num_targets / num_targets\n",
    "              break\n",
    "\n",
    "        # include a penalty to appropriately offset the MR\n",
    "        # rather than artifically inflating the performance\n",
    "        # by simply dropping queries that do not contain \n",
    "        # (unlikely in this case)\n",
    "        if rank_pos is None:\n",
    "            results[\"MR\"] += MAX_K + 1 # penalty: rank := MAX_K + 1\n",
    "\n",
    "        for k in ks:\n",
    "            hit = 1 if (rank_pos is not None and rank_pos <= k) else 0\n",
    "            results[f\"Hits@{k}\"] += hit\n",
    "            top_k_results = set(retrieved_iris[:k])\n",
    "            total_hits_at_k = len(g_target_iris.intersection(top_k_results))\n",
    "            p_at_k = total_hits_at_k / k # Precision@K\n",
    "            results[f\"P@{k}\"] += p_at_k\n",
    "            r_at_k = total_hits_at_k / num_targets\n",
    "            results[f\"R@{k}\"] += r_at_k\n",
    "            if (p_at_k + r_at_k) > 0:\n",
    "               results[f\"F1@{k}\"] += 2 * (p_at_k * r_at_k) / (p_at_k + r_at_k)\n",
    "            iDCG, targets_with_dcg = query.get_targets_with_dcg(type=\"exp\", depth_cutoff=global_cutoff_depth)\n",
    "            results[f\"nDCG@{k}\"] += compute_ndcg_at_k(ranked_results, targets_with_dcg, k) # type: ignore\n",
    "\n",
    "        final_rank = rank_pos if rank_pos is not None else MAX_K + 1\n",
    "        all_ranks.append(final_rank)\n",
    "\n",
    "    # (macro) PR-AUC\n",
    "    R_grid, P_macro = macro_pr_curve(per_query_rels_for_PR, recall_points=101)\n",
    "    macro_pr_auc = float(np.trapezoid(P_macro, R_grid))\n",
    "\n",
    "    # normalise over queries & compute coverage\n",
    "    N = len(oov_match_all)\n",
    "    normalized = {metric: value / N for metric, value in results.items()}\n",
    "    normalized['Cov'] = (hit_count / total_possible_hits) # calculate the coverage of this model\n",
    "    normalized['Med'] = statistics.median(all_ranks) # median rank\n",
    "    # area under precision-recall curve (trapezodial rule)\n",
    "    recall_at_k_xs    = [normalized[f\"R@{k}\"] for k in ks]\n",
    "    # check for monotonic recall\n",
    "    if any(r2 < r1 for r1, r2 in zip(recall_at_k_xs, recall_at_k_xs[1:])):\n",
    "        raise ValueError(f\"Recall must be non-decreasing for PR-AUC\")\n",
    "    precision_at_k_xs = [normalized[f\"P@{k}\"] for k in ks]\n",
    "    normalized[\"AUC\"] = float(sk_auc(recall_at_k_xs, precision_at_k_xs))\n",
    "    normalized[\"MacroPR_AUC\"] = macro_pr_auc\n",
    "\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\" mAP: \\t  {normalized['MAP']:.2f}\") # Mean Average Precision\n",
    "    print(f\" MRR*:   {normalized['MRR']:.2f}\") # MRR at first hit ranks\n",
    "    print(f\" nDCG@10: {normalized['nDCG@10']:.2f}\") # nDCG@10\n",
    "    print(f\" PR-AUC:  {normalized['AUC']:.2f}\") # area under precision-recall curve\n",
    "    print(f\" mPR-AUC: {normalized['MacroPR_AUC']:.2f}\") # PR-AUC (macro averaged)\n",
    "    print(f\" R@100:   {normalized['R@100']:.2f}\") # Recall@100\n",
    "    print(\"-\"*60)\n",
    "\n",
    "    all_results[model_name] = normalized\n",
    "\n",
    "    model_metric_string = model_name.split()\n",
    "\n",
    "    experiment_three_table.add_row([\n",
    "      model_metric_string[0],\n",
    "      model_metric_string[1],\n",
    "      model_metric_string[2],\n",
    "      normalized['MAP'], \n",
    "      normalized['MRR'], \n",
    "      normalized['nDCG@10'], \n",
    "      normalized['MacroPR_AUC'], # normalized['AUC'],\n",
    "      normalized['R@100']\n",
    "    ])\n",
    "\n",
    "    macro_avg_PR_AUC_data[model_name] = {\n",
    "      \"recall\": R_grid.tolist(),\n",
    "      \"precision\": P_macro.tolist()\n",
    "    }\n",
    "\n",
    "Path('../logs').mkdir(parents=True, exist_ok=True)\n",
    "output_file = '../logs/oov_entity_mentions_multi_relevant_targets_weight_w035_50_queries.json'\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "\n",
    "print(f\"All results saved to {output_file}\")\n",
    "\n",
    "output_macro_pr_auc_file = '../logs/oov_entity_mentions_multi_target_WEIGHTED_w035_50_q__PR_AUC_POINTS_4_PLOT.json'\n",
    "with open(output_macro_pr_auc_file, 'w') as f:\n",
    "    json.dump(macro_avg_PR_AUC_data, f, indent=2)\n",
    "\n",
    "print(f\"Macro PR AUC plot data dumped to {output_macro_pr_auc_file}\")\n",
    "\n",
    "print(f\"Printing table: \\n\\n\")\n",
    "\n",
    "print(experiment_three_table.draw())\n",
    "\n",
    "print(\"\\n\\n Printing LaTeX: \\n\\n\")\n",
    "\n",
    "print(latextable.draw_latex(\n",
    "    table=experiment_three_table, \n",
    "    caption=\"Performance of fetching multiple relevant entities using OOV mentions with lambda=0.35 (50 Queries)\",\n",
    "    use_booktabs=True, position=\"H\", caption_above=True, caption_short=\"Multi target performance of OOV mentions, lambda=0.35\",\n",
    "    label=\"tab:multi-target-oov-weighted\"\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad02e0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "knowledge-retrieval-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
